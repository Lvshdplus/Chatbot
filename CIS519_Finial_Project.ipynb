{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLEFunpdhlzL"
   },
   "source": [
    "# 1. Data Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zw0TQGqWh_cY"
   },
   "source": [
    "## 1.1 Data Collection \n",
    "\n",
    "Download the datasets from [Cornell Movie Datasets Website](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) and unzip the data into txt files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hQ08H1wEiHzD",
    "outputId": "446f43b5-e33e-4a72-8fdc-a01053dfa42e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-19 19:23:37--  http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
      "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
      "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9916637 (9.5M) [application/zip]\n",
      "Saving to: ‘cornell_movie_dialogs_corpus.zip’\n",
      "\n",
      "cornell_movie_dialo 100%[===================>]   9.46M  58.6MB/s    in 0.2s    \n",
      "\n",
      "2022-04-19 19:23:37 (58.6 MB/s) - ‘cornell_movie_dialogs_corpus.zip’ saved [9916637/9916637]\n",
      "\n",
      "Archive:  cornell_movie_dialogs_corpus.zip\n",
      "   creating: cornell movie-dialogs corpus/\n",
      "  inflating: cornell movie-dialogs corpus/.DS_Store  \n",
      "   creating: __MACOSX/\n",
      "   creating: __MACOSX/cornell movie-dialogs corpus/\n",
      "  inflating: __MACOSX/cornell movie-dialogs corpus/._.DS_Store  \n",
      "  inflating: cornell movie-dialogs corpus/chameleons.pdf  \n",
      "  inflating: __MACOSX/cornell movie-dialogs corpus/._chameleons.pdf  \n",
      "  inflating: cornell movie-dialogs corpus/movie_characters_metadata.txt  \n",
      "  inflating: cornell movie-dialogs corpus/movie_conversations.txt  \n",
      "  inflating: cornell movie-dialogs corpus/movie_lines.txt  \n",
      "  inflating: cornell movie-dialogs corpus/movie_titles_metadata.txt  \n",
      "  inflating: cornell movie-dialogs corpus/raw_script_urls.txt  \n",
      "  inflating: cornell movie-dialogs corpus/README.txt  \n",
      "  inflating: __MACOSX/cornell movie-dialogs corpus/._README.txt  \n"
     ]
    }
   ],
   "source": [
    "! wget -nc \"http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\"\n",
    "! unzip cornell_movie_dialogs_corpus.zip\n",
    "! rm cornell_movie_dialogs_corpus.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwM5YMC1QXH_"
   },
   "source": [
    "## 1.2 Data Cleaning & Wrangling\n",
    "\n",
    "Clean the data and convert it into the form of a dialog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Bbp11GwnmtYG"
   },
   "outputs": [],
   "source": [
    "# open dialog files\n",
    "movie_lines = open('cornell movie-dialogs corpus/movie_lines.txt', encoding='utf-8',errors='ignore').read().split('\\n')\n",
    "movie_conversations = open('cornell movie-dialogs corpus/movie_conversations.txt', encoding='utf-8',errors='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yaSnbTgHnnwH"
   },
   "outputs": [],
   "source": [
    "# build a dictionary to record (line_number, dialog) mappings\n",
    "line_to_dialog = {}\n",
    "for line in movie_lines:\n",
    "  line_splited = line.split(' +++$+++ ')\n",
    "  line_to_dialog[line_splited[0]] = line_splited[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ktu9maMdp5YE"
   },
   "outputs": [],
   "source": [
    "# build dialog fragments\n",
    "dialog_fragments = []\n",
    "for conversation in movie_conversations:\n",
    "  ## convert u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197'] to 'L194', 'L195', 'L196', 'L197'\n",
    "  dialog_instance = conversation.split(' +++$+++ ')[-1][1:-1]\n",
    "  ## convert 'L194', 'L195', 'L196', 'L197' to ['L194', 'L195', 'L196', 'L197']\n",
    "  dialog_fragments.append(dialog_instance.replace(\"'\", \" \").replace(\",\",\"\").split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "n6ZyUbPJk6xE"
   },
   "outputs": [],
   "source": [
    "# convert dialog fragments into (question, answer) pairs\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for frag in dialog_fragments:\n",
    "    for i in range(1, len(frag)):\n",
    "        questions.append(line_to_dialog[frag[i-1]])\n",
    "        answers.append(line_to_dialog[frag[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sAWPaVbtmdX4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "\n",
      "Dialog A: Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "\n",
      "Dialog B: Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "Dialog A: Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "\n",
      "Dialog B: Not the hacking and gagging and spitting part.  Please.\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "Dialog A: Not the hacking and gagging and spitting part.  Please.\n",
      "\n",
      "Dialog B: Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "Dialog A: You're asking me out.  That's so cute. What's your name again?\n",
      "\n",
      "Dialog B: Forget it.\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "Dialog A: No, no, it's my fault -- we didn't have a proper introduction ---\n",
      "\n",
      "Dialog B: Cameron.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show questions and answers pairs\n",
    "def qa_show(num):\n",
    "  for i in range(num):\n",
    "    print('-------------------------------------------------\\n')\n",
    "    print(f\"Dialog A: {questions[i]}\\n\")\n",
    "    print(f\"Dialog B: {answers[i]}\\n\")\n",
    "\n",
    "qa_show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixNAnkQiQ6wy"
   },
   "source": [
    "## 1.3 Text Preprocessing\n",
    "\n",
    "Preprocess the texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XkeekSzCSEfn"
   },
   "outputs": [],
   "source": [
    "# Global variables\n",
    "TEXT_LIMIT = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9WZ0gkXaSNNK"
   },
   "outputs": [],
   "source": [
    "# filter out long dialogs\n",
    "def filter_long_texts(questions, answers, limit):\n",
    "    short_questions = []\n",
    "    short_answers = []\n",
    "    for i in range(len(questions)):\n",
    "        # if len(questions[i]) <= TEXT_LIMIT and len(answers[i]) <=TEXT_LIMIT:\n",
    "        if len(questions[i].split()) <= TEXT_LIMIT:\n",
    "            short_questions.append(questions[i])\n",
    "            short_answers.append(answers[i])\n",
    "    return short_questions, short_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Ix7oU7nabaun"
   },
   "outputs": [],
   "source": [
    "filtered_questions, filtered_answers = filter_long_texts(questions, answers, TEXT_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pY6jluMCSsn8"
   },
   "outputs": [],
   "source": [
    "# clean the texts\n",
    "import re\n",
    "\n",
    "replacement_patterns = [\n",
    "  (r'won\\'t', 'will not'),\n",
    "  (r'can\\'t', 'cannot'),\n",
    "  (r'i\\'m', 'i am'),\n",
    "  (r'ain\\'t', 'is not'),\n",
    "  (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "  (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "  (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "  (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "  (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "  (r'(\\w+)\\'d', '\\g<1> would'),\n",
    "]\n",
    "\n",
    "class TextCleaner(object):\n",
    "  def __init__(self, patterns=replacement_patterns):\n",
    "    self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "    \n",
    "  def replace(self, text):\n",
    "    s = text\n",
    "    for (pattern, replace) in self.patterns:\n",
    "      s = re.sub(pattern, replace, s)\n",
    "    return s\n",
    "\n",
    "cleaner = TextCleaner()\n",
    "\n",
    "# Function for preprocessing the given text\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Decontracting the text (e.g. it's -> it is)\n",
    "    text = cleaner.replace(text)\n",
    "    \n",
    "    # Remove the punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r\"[ ]+\", \" \", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XcPRHsfIXDtc",
    "outputId": "2a4561f0-d095-4834-c021-f9fbbb1cbfb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "\n",
      "Dialog A: well i thought we would start with pronunciation if that is okay with you \n",
      "\n",
      "Dialog B: Not the hacking and gagging and spitting part. Please.\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "Dialog A: not the hacking and gagging and spitting part please \n",
      "\n",
      "Dialog B: Okay... then how 'bout we try out some French cuisine. Saturday? Night?\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "Dialog A: you are asking me out that is so cute what is your name again \n",
      "\n",
      "Dialog B: Forget it.\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "Dialog A: no no it is my fault we did not have a proper introduction \n",
      "\n",
      "Dialog B: Cameron.\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "Dialog A: cameron \n",
      "\n",
      "Dialog B: The thing is, Cameron -- I'm at the mercy of a particularly hideous\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_questions = [preprocess_text(que) for que in filtered_questions]\n",
    "cleaned_answers = [' '.join(ans.split()[:TEXT_LIMIT-2]) for ans in filtered_answers]\n",
    "\n",
    "del(questions, answers, filtered_questions, filtered_answers)\n",
    "\n",
    "for i in range(5):\n",
    "    print('-------------------------------------------------\\n')\n",
    "    print(f'Dialog A: {cleaned_questions[i]}\\n')\n",
    "    print(f'Dialog B: {cleaned_answers[i]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEuvFeGuF9bq"
   },
   "source": [
    "After preprocessing the dataset, we should add a start tag (e.g. `<start>`) and an end tag (e.g. `<end>`) to answers. Remember that we will only add these tags to answers and not questions.\n",
    "\n",
    "When Seq2Seq model is generating the word answers, we can first send it the `<start>` to begin the word generation. When `<end>` is generated, we will stop the iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ETc_LE9sXX0O"
   },
   "outputs": [],
   "source": [
    "cleaned_answers = [\"starttoken \" + ans + \" endtoken\" for ans in cleaned_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lakxrfs9H0uQ"
   },
   "outputs": [],
   "source": [
    "# trim the data in case of running out of RAM\n",
    "# TRAINING_SIZE = 170000\n",
    "training_questions = cleaned_questions\n",
    "training_answers = cleaned_answers\n",
    "# training_questions = cleaned_questions[:TRAINING_SIZE]\n",
    "# training_answers = cleaned_answers[:TRAINING_SIZE]\n",
    "\n",
    "# testing_questions = cleaned_questions[TRAINING_SIZE:]\n",
    "# testing_answers = cleaned_answers[TRAINING_SIZE:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pj2VSg6ento0"
   },
   "source": [
    "## 1.4 Input Encoding\n",
    "\n",
    "Convet String input into numerical values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6B4lo51bH6Fs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing, utils\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "CJUf6CjJ37u7"
   },
   "outputs": [],
   "source": [
    "NUM_WORDS = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TjSxe5uGIH6q",
    "outputId": "ed256e97-3f6c-407f-f305-bebc54a9616d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE : 41737\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = preprocessing.text.Tokenizer(num_words = NUM_WORDS, oov_token='nulltoken')\n",
    "\n",
    "# Fit the tokenizer to questions and answers\n",
    "tokenizer.fit_on_texts(training_questions + training_answers)\n",
    "\n",
    "# Get the total vocab size\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "# display(tokenizer.word_index)\n",
    "print( 'VOCAB SIZE : {}'.format(VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XGTAwFiL9Wq3",
    "outputId": "d45d2a0c-0397-4ef5-9958-2ca5bda07bdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index[\"starttoken\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aCeEQiMwIZas",
    "outputId": "dd38922f-dd4d-4f57-c6d7-2e94f4e62d4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(179880, 15) 15\n"
     ]
    }
   ],
   "source": [
    "### encoder input data\n",
    "\n",
    "# Tokenize the questions\n",
    "tokenized_questions_training = tokenizer.texts_to_sequences(training_questions)\n",
    "# tokenized_questions_testing = tokenizer.texts_to_sequences(testing_questions)\n",
    "\n",
    "# Pad the sequences\n",
    "padded_questions_training = pad_sequences(tokenized_questions_training, maxlen=TEXT_LIMIT, padding='post')\n",
    "# padded_questions_testing = pad_sequences(tokenized_questions_testing, maxlen=TEXT_LIMIT, padding='post')\n",
    "\n",
    "# Convert the sequences into array\n",
    "encoder_input_data_training = np.array(padded_questions_training)\n",
    "print(encoder_input_data_training.shape, TEXT_LIMIT)\n",
    "# encoder_input_data_testing = np.array(padded_questions_testing)\n",
    "# print(encoder_input_data_testing.shape, TEXT_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uoKg2PHoIkcH",
    "outputId": "092abd9d-dba9-4bc6-e23e-ec6a01e7f418"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(179880, 15) 15\n"
     ]
    }
   ],
   "source": [
    "### decoder input data\n",
    "\n",
    "# Tokenize the answers\n",
    "tokenized_answers_training = tokenizer.texts_to_sequences(training_answers)\n",
    "# tokenized_answers_testing = tokenizer.texts_to_sequences(testing_answers)\n",
    "\n",
    "# Pad the sequences\n",
    "padded_answers_training = pad_sequences(tokenized_answers_training, maxlen=TEXT_LIMIT, padding='post')\n",
    "# padded_answers_testing = pad_sequences(tokenized_answers_testing, maxlen=TEXT_LIMIT, padding='post')\n",
    "\n",
    "# Convert the sequences into array\n",
    "decoder_input_data_training = np.array(padded_answers_training)\n",
    "print(decoder_input_data_training.shape, TEXT_LIMIT)\n",
    "\n",
    "# decoder_input_data_testing = np.array(padded_answers_testing)\n",
    "# print(decoder_input_data_testing.shape, TEXT_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "SXi3CTOMI8AX"
   },
   "outputs": [],
   "source": [
    "### decoder_output_data\n",
    "\n",
    "# Iterate through index of tokenized answers\n",
    "for i in range(len(tokenized_answers_training)):\n",
    "    tokenized_answers_training[i] = tokenized_answers_training[i][1:]\n",
    "\n",
    "# for i in range(len(tokenized_answers_testing)):\n",
    "#     tokenized_answers_testing[i] = tokenized_answers_testing[i][1:]\n",
    "\n",
    "# Pad the tokenized answers\n",
    "padded_answers_training = pad_sequences(tokenized_answers_training, maxlen = TEXT_LIMIT, padding = 'post')\n",
    "# padded_answers_testing = pad_sequences(tokenized_answers_testing, maxlen = TEXT_LIMIT, padding = 'post')\n",
    "\n",
    "# One hot encode\n",
    "onehot_answers_training = utils.to_categorical(padded_answers_training, NUM_WORDS+1)\n",
    "# onehot_answers_testing = utils.to_categorical(padded_answers_testing, NUM_WORDS+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZnyhJ7VC4-YY",
    "outputId": "03520b6b-3622-4ef4-c6b8-8c636972f595"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(179880, 15, 8001)\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy array\n",
    "decoder_output_data_training = np.array(onehot_answers_training)\n",
    "del(onehot_answers_training)\n",
    "\n",
    "# decoder_output_data_testing = np.array(onehot_answers_testing)\n",
    "# del (onehot_answers_testing)\n",
    "\n",
    "print(decoder_output_data_training.shape)\n",
    "# print(decoder_output_data_testing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ZmOuXZ6vJJcj"
   },
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  ['/job:localhost/replica:0/task:0/device:CPU:0', '/job:localhost/replica:0/task:0/device:XLA_CPU:0', '/job:localhost/replica:0/task:0/device:XLA_GPU:0', '/job:localhost/replica:0/task:0/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", tf.config.experimental_list_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "SMch0EsnkmMN"
   },
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 120\n",
    "VOCAB_SIZE = NUM_WORDS + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Wwi7M2Nmk1oR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "### Encoder Input\n",
    "embed_dim = 50\n",
    "num_lstm = 400\n",
    "\n",
    "# Input for encoder\n",
    "encoder_inputs = Input(shape = (None, ), name='encoder_inputs')\n",
    "\n",
    "# Embedding layer\n",
    "# Why mask_zero = True? https://www.tensorflow.org/guide/keras/masking_and_padding\n",
    "encoder_embedding = Embedding(input_dim = VOCAB_SIZE, output_dim = embed_dim, mask_zero = True, name='encoder_embedding')(encoder_inputs)\n",
    "\n",
    "# LSTM layer (that returns states in addition to output)\n",
    "encoder_outputs, state_h, state_c = LSTM(units = num_lstm, return_state = True, name='encoder_lstm')(encoder_embedding)\n",
    "\n",
    "# Get the states for encoder\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "wbyKdEcXk6d6"
   },
   "outputs": [],
   "source": [
    "### Decoder\n",
    "\n",
    "# Input for decoder\n",
    "decoder_inputs = Input(shape = (None,  ), name='decoder_inputs')\n",
    "\n",
    "# Embedding layer\n",
    "decoder_embedding = Embedding(input_dim = VOCAB_SIZE, output_dim = embed_dim , mask_zero = True, name='decoder_embedding')(decoder_inputs)\n",
    "\n",
    "# LSTM layer (that returns states and sequences as well)\n",
    "decoder_lstm = LSTM(units = num_lstm , return_state = True , return_sequences = True, name='decoder_lstm')\n",
    "\n",
    "# Get the output of LSTM layer, using the initial states from the encoder\n",
    "decoder_outputs, _, _ = decoder_lstm(inputs = decoder_embedding, initial_state = encoder_states)\n",
    "\n",
    "# Dense layer\n",
    "decoder_dense = Dense(units = VOCAB_SIZE, activation = softmax, name='decoder_outputs') \n",
    "\n",
    "# Get the output of Dense layer\n",
    "output = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "xK6yhoCtlDYK"
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Model([encoder_inputs, decoder_inputs], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "T_nMCQ3VlHoO"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', metrics=['acc'], loss = \"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bHJ7aT6GlH4x",
    "outputId": "b5d49285-e393-4cc8-dc20-b70ea2d291f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedding (Embedding)   (None, None, 50)     400050      encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedding (Embedding)   (None, None, 50)     400050      decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm (LSTM)             [(None, 400), (None, 721600      encoder_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 400),  721600      decoder_embedding[0][0]          \n",
      "                                                                 encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_outputs (Dense)         (None, None, 8001)   3208401     decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 5,451,701\n",
      "Trainable params: 5,451,701\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IfVO8dDhlNeH",
    "outputId": "b5f08152-316c-4bf4-e01d-7dd2be23ff72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:163: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:189: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "[2022-04-21 04:19:22.011 ip-172-16-68-67:2940 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2022-04-21 04:19:22.035 ip-172-16-68-67:2940 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "Train on 161892 samples, validate on 17988 samples\n",
      "Epoch 1/120\n",
      "161892/161892 [==============================] - 322s 2ms/sample - loss: 2.8405 - acc: 0.2681 - val_loss: 2.6824 - val_acc: 0.2997\n",
      "Epoch 2/120\n",
      "161892/161892 [==============================] - 322s 2ms/sample - loss: 2.5323 - acc: 0.3100 - val_loss: 2.5641 - val_acc: 0.3198\n",
      "Epoch 3/120\n",
      "161892/161892 [==============================] - 314s 2ms/sample - loss: 2.4110 - acc: 0.3246 - val_loss: 2.5231 - val_acc: 0.3280\n",
      "Epoch 4/120\n",
      "161892/161892 [==============================] - 310s 2ms/sample - loss: 2.3249 - acc: 0.3336 - val_loss: 2.5097 - val_acc: 0.3338\n",
      "Epoch 5/120\n",
      "161892/161892 [==============================] - 313s 2ms/sample - loss: 2.2542 - acc: 0.3408 - val_loss: 2.5084 - val_acc: 0.3359\n",
      "Epoch 6/120\n",
      "161892/161892 [==============================] - 314s 2ms/sample - loss: 2.1934 - acc: 0.3469 - val_loss: 2.5190 - val_acc: 0.3364\n",
      "Epoch 7/120\n",
      "161892/161892 [==============================] - 310s 2ms/sample - loss: 2.1397 - acc: 0.3522 - val_loss: 2.5311 - val_acc: 0.3368\n",
      "Epoch 8/120\n",
      "161892/161892 [==============================] - 310s 2ms/sample - loss: 2.0907 - acc: 0.3576 - val_loss: 2.5468 - val_acc: 0.3366\n",
      "Epoch 9/120\n",
      "161892/161892 [==============================] - 318s 2ms/sample - loss: 2.0459 - acc: 0.3632 - val_loss: 2.5648 - val_acc: 0.3353\n",
      "Epoch 10/120\n",
      "161892/161892 [==============================] - 315s 2ms/sample - loss: 2.0038 - acc: 0.3690 - val_loss: 2.5821 - val_acc: 0.3349\n",
      "Epoch 11/120\n",
      "161892/161892 [==============================] - 317s 2ms/sample - loss: 1.9644 - acc: 0.3751 - val_loss: 2.6061 - val_acc: 0.3326\n",
      "Epoch 12/120\n",
      "161892/161892 [==============================] - 320s 2ms/sample - loss: 1.9274 - acc: 0.3810 - val_loss: 2.6246 - val_acc: 0.3305\n",
      "Epoch 13/120\n",
      "161892/161892 [==============================] - 317s 2ms/sample - loss: 1.8926 - acc: 0.3872 - val_loss: 2.6443 - val_acc: 0.3298\n",
      "Epoch 14/120\n",
      "161892/161892 [==============================] - 318s 2ms/sample - loss: 1.7062 - acc: 0.4249 - val_loss: 2.7807 - val_acc: 0.3178\n",
      "Epoch 21/120\n",
      "161892/161892 [==============================] - 318s 2ms/sample - loss: 1.6863 - acc: 0.4293 - val_loss: 2.8010 - val_acc: 0.3177\n",
      "Epoch 22/120\n",
      "161892/161892 [==============================] - 316s 2ms/sample - loss: 1.6681 - acc: 0.4333 - val_loss: 2.8172 - val_acc: 0.3154\n",
      "Epoch 23/120\n",
      "161892/161892 [==============================] - 317s 2ms/sample - loss: 1.6507 - acc: 0.4374 - val_loss: 2.8348 - val_acc: 0.3128\n",
      "Epoch 24/120\n",
      "161892/161892 [==============================] - 318s 2ms/sample - loss: 1.6353 - acc: 0.4409 - val_loss: 2.8480 - val_acc: 0.3129\n",
      "Epoch 25/120\n",
      "161892/161892 [==============================] - 317s 2ms/sample - loss: 1.6208 - acc: 0.4439 - val_loss: 2.8632 - val_acc: 0.3125\n",
      "Epoch 26/120\n",
      "161892/161892 [==============================] - 317s 2ms/sample - loss: 1.6072 - acc: 0.4472 - val_loss: 2.8783 - val_acc: 0.3129\n",
      "Epoch 27/120\n",
      "161892/161892 [==============================] - 318s 2ms/sample - loss: 1.5949 - acc: 0.4498 - val_loss: 2.8907 - val_acc: 0.3113\n",
      "Epoch 28/120\n",
      "161892/161892 [==============================] - 316s 2ms/sample - loss: 1.5826 - acc: 0.4527 - val_loss: 2.9065 - val_acc: 0.3102\n",
      "Epoch 29/120\n",
      "161892/161892 [==============================] - 319s 2ms/sample - loss: 1.5721 - acc: 0.4553 - val_loss: 2.9201 - val_acc: 0.3079\n",
      "Epoch 30/120\n",
      "161892/161892 [==============================] - 318s 2ms/sample - loss: 1.5616 - acc: 0.4571 - val_loss: 2.9290 - val_acc: 0.3066\n",
      "Epoch 31/120\n",
      "161892/161892 [==============================] - 317s 2ms/sample - loss: 1.5518 - acc: 0.4594 - val_loss: 2.9382 - val_acc: 0.3078\n",
      "Epoch 32/120\n",
      "161892/161892 [==============================] - 317s 2ms/sample - loss: 1.5427 - acc: 0.4616 - val_loss: 2.9516 - val_acc: 0.3057\n",
      "Epoch 33/120\n",
      "161892/161892 [==============================] - 313s 2ms/sample - loss: 1.5351 - acc: 0.4633 - val_loss: 2.9650 - val_acc: 0.3047\n",
      "Epoch 34/120\n",
      "161892/161892 [==============================] - 314s 2ms/sample - loss: 1.5269 - acc: 0.4651 - val_loss: 2.9712 - val_acc: 0.3042\n",
      "Epoch 35/120\n",
      "161892/161892 [==============================] - 316s 2ms/sample - loss: 1.5197 - acc: 0.4666 - val_loss: 2.9836 - val_acc: 0.3047\n",
      "Epoch 36/120\n",
      "161892/161892 [==============================] - 320s 2ms/sample - loss: 1.5122 - acc: 0.4685 - val_loss: 2.9928 - val_acc: 0.3006\n",
      "Epoch 37/120\n",
      "161892/161892 [==============================] - 319s 2ms/sample - loss: 1.5063 - acc: 0.4696 - val_loss: 2.9987 - val_acc: 0.3048\n",
      "Epoch 38/120\n",
      "161892/161892 [==============================] - 314s 2ms/sample - loss: 1.5002 - acc: 0.4709 - val_loss: 3.0096 - val_acc: 0.3012\n",
      "Epoch 39/120\n",
      "161892/161892 [==============================] - 315s 2ms/sample - loss: 1.4941 - acc: 0.4723 - val_loss: 3.0157 - val_acc: 0.3024\n",
      "Epoch 40/120\n",
      "161892/161892 [==============================] - 319s 2ms/sample - loss: 1.4888 - acc: 0.4737 - val_loss: 3.0219 - val_acc: 0.3007\n",
      "Epoch 41/120\n",
      "161892/161892 [==============================] - 316s 2ms/sample - loss: 1.4843 - acc: 0.4746 - val_loss: 3.0315 - val_acc: 0.3004\n",
      "Epoch 42/120\n",
      "161892/161892 [==============================] - 319s 2ms/sample - loss: 1.4792 - acc: 0.4751 - val_loss: 3.0413 - val_acc: 0.3015\n",
      "Epoch 43/120\n",
      "161892/161892 [==============================] - 320s 2ms/sample - loss: 1.4747 - acc: 0.4763 - val_loss: 3.0412 - val_acc: 0.2994\n",
      "Epoch 44/120\n",
      "161892/161892 [==============================] - 314s 2ms/sample - loss: 1.4707 - acc: 0.4771 - val_loss: 3.0502 - val_acc: 0.3004\n",
      "Epoch 45/120\n",
      "161892/161892 [==============================] - 313s 2ms/sample - loss: 1.4660 - acc: 0.4785 - val_loss: 3.0600 - val_acc: 0.2991\n",
      "Epoch 46/120\n",
      "161892/161892 [==============================] - 314s 2ms/sample - loss: 1.4630 - acc: 0.4787 - val_loss: 3.0680 - val_acc: 0.2988\n",
      "Epoch 47/120\n",
      "161892/161892 [==============================] - 324s 2ms/sample - loss: 1.4588 - acc: 0.4801 - val_loss: 3.0708 - val_acc: 0.2997\n",
      "Epoch 48/120\n",
      "161892/161892 [==============================] - 323s 2ms/sample - loss: 1.4560 - acc: 0.4809 - val_loss: 3.0755 - val_acc: 0.2987\n",
      "Epoch 49/120\n",
      "161892/161892 [==============================] - 318s 2ms/sample - loss: 1.4520 - acc: 0.4810 - val_loss: 3.0824 - val_acc: 0.2989\n",
      "Epoch 50/120\n",
      "161892/161892 [==============================] - 314s 2ms/sample - loss: 1.4495 - acc: 0.4815 - val_loss: 3.0864 - val_acc: 0.2986\n",
      "Epoch 51/120\n",
      "161892/161892 [==============================] - 315s 2ms/sample - loss: 1.4468 - acc: 0.4823 - val_loss: 3.0926 - val_acc: 0.2986\n",
      "Epoch 52/120\n",
      "161892/161892 [==============================] - 315s 2ms/sample - loss: 1.4435 - acc: 0.4831 - val_loss: 3.0951 - val_acc: 0.2991\n",
      "Epoch 53/120\n",
      "161892/161892 [==============================] - 312s 2ms/sample - loss: 1.4405 - acc: 0.4835 - val_loss: 3.1012 - val_acc: 0.2999\n",
      "Epoch 54/120\n",
      "161892/161892 [==============================] - 314s 2ms/sample - loss: 1.4382 - acc: 0.4840 - val_loss: 3.1023 - val_acc: 0.2981\n",
      "Epoch 55/120\n",
      "161892/161892 [==============================] - 311s 2ms/sample - loss: 1.4352 - acc: 0.4851 - val_loss: 3.1071 - val_acc: 0.2977\n",
      "Epoch 56/120\n",
      "161892/161892 [==============================] - 312s 2ms/sample - loss: 1.4337 - acc: 0.4850 - val_loss: 3.1143 - val_acc: 0.2977\n",
      "Epoch 57/120\n",
      "161892/161892 [==============================] - 313s 2ms/sample - loss: 1.4314 - acc: 0.4858 - val_loss: 3.1205 - val_acc: 0.2968\n",
      "Epoch 58/120\n",
      "161892/161892 [==============================] - 315s 2ms/sample - loss: 1.4291 - acc: 0.4861 - val_loss: 3.1224 - val_acc: 0.2958\n",
      "Epoch 59/120\n",
      "161892/161892 [==============================] - 314s 2ms/sample - loss: 1.4269 - acc: 0.4864 - val_loss: 3.1273 - val_acc: 0.2945\n",
      "Epoch 60/120\n",
      "161892/161892 [==============================] - 316s 2ms/sample - loss: 1.4253 - acc: 0.4868 - val_loss: 3.1319 - val_acc: 0.2947\n",
      "Epoch 61/120\n",
      "161892/161892 [==============================] - 313s 2ms/sample - loss: 1.4227 - acc: 0.4872 - val_loss: 3.1338 - val_acc: 0.2959\n",
      "Epoch 62/120\n",
      "161892/161892 [==============================] - 312s 2ms/sample - loss: 1.4210 - acc: 0.4877 - val_loss: 3.1358 - val_acc: 0.2957\n",
      "Epoch 63/120\n",
      "161892/161892 [==============================] - 310s 2ms/sample - loss: 1.4196 - acc: 0.4879 - val_loss: 3.1397 - val_acc: 0.2949\n",
      "Epoch 64/120\n",
      "161892/161892 [==============================] - 318s 2ms/sample - loss: 1.4181 - acc: 0.4882 - val_loss: 3.1430 - val_acc: 0.2939\n",
      "Epoch 65/120\n",
      "161892/161892 [==============================] - 314s 2ms/sample - loss: 1.4158 - acc: 0.4887 - val_loss: 3.1436 - val_acc: 0.2930\n",
      "Epoch 66/120\n",
      "161892/161892 [==============================] - 314s 2ms/sample - loss: 1.4152 - acc: 0.4886 - val_loss: 3.1525 - val_acc: 0.2939\n",
      "Epoch 67/120\n",
      "161892/161892 [==============================] - 312s 2ms/sample - loss: 1.4140 - acc: 0.4893 - val_loss: 3.1574 - val_acc: 0.2955\n",
      "Epoch 68/120\n",
      "161892/161892 [==============================] - 313s 2ms/sample - loss: 1.4102 - acc: 0.4898 - val_loss: 3.1559 - val_acc: 0.2959\n",
      "Epoch 69/120\n",
      "161892/161892 [==============================] - 322s 2ms/sample - loss: 1.4106 - acc: 0.4894 - val_loss: 3.1620 - val_acc: 0.2954\n",
      "Epoch 70/120\n",
      "161892/161892 [==============================] - 318s 2ms/sample - loss: 1.4084 - acc: 0.4903 - val_loss: 3.1578 - val_acc: 0.2972\n",
      "Epoch 71/120\n",
      "161892/161892 [==============================] - 315s 2ms/sample - loss: 1.4083 - acc: 0.4901 - val_loss: 3.1655 - val_acc: 0.2935\n",
      "Epoch 72/120\n",
      "161892/161892 [==============================] - 311s 2ms/sample - loss: 1.4069 - acc: 0.4902 - val_loss: 3.1700 - val_acc: 0.2939\n",
      "Epoch 73/120\n",
      "161892/161892 [==============================] - 309s 2ms/sample - loss: 1.4055 - acc: 0.4909 - val_loss: 3.1729 - val_acc: 0.2944\n",
      "Epoch 74/120\n",
      "161892/161892 [==============================] - 318s 2ms/sample - loss: 1.4044 - acc: 0.4906 - val_loss: 3.1722 - val_acc: 0.2941\n",
      "Epoch 75/120\n",
      "161892/161892 [==============================] - 319s 2ms/sample - loss: 1.4035 - acc: 0.4908 - val_loss: 3.1782 - val_acc: 0.2929\n",
      "Epoch 76/120\n",
      "161892/161892 [==============================] - 314s 2ms/sample - loss: 1.4021 - acc: 0.4910 - val_loss: 3.1792 - val_acc: 0.2932\n",
      "Epoch 77/120\n",
      "161892/161892 [==============================] - 317s 2ms/sample - loss: 1.4018 - acc: 0.4913 - val_loss: 3.1819 - val_acc: 0.2921\n",
      "Epoch 78/120\n",
      "161892/161892 [==============================] - 318s 2ms/sample - loss: 1.4005 - acc: 0.4916 - val_loss: 3.1831 - val_acc: 0.2933\n",
      "Epoch 79/120\n",
      "161892/161892 [==============================] - 313s 2ms/sample - loss: 1.3992 - acc: 0.4915 - val_loss: 3.1854 - val_acc: 0.2926\n",
      "Epoch 80/120\n",
      "161892/161892 [==============================] - 313s 2ms/sample - loss: 1.3982 - acc: 0.4919 - val_loss: 3.1903 - val_acc: 0.2933\n",
      "Epoch 81/120\n",
      "161892/161892 [==============================] - 315s 2ms/sample - loss: 1.3970 - acc: 0.4921 - val_loss: 3.1912 - val_acc: 0.2937\n",
      "Epoch 82/120\n",
      "161892/161892 [==============================] - 313s 2ms/sample - loss: 1.3973 - acc: 0.4920 - val_loss: 3.1860 - val_acc: 0.2943\n",
      "Epoch 83/120\n",
      "161892/161892 [==============================] - 315s 2ms/sample - loss: 1.3955 - acc: 0.4926 - val_loss: 3.1918 - val_acc: 0.2925\n",
      "Epoch 84/120\n",
      "161892/161892 [==============================] - 313s 2ms/sample - loss: 1.3956 - acc: 0.4922 - val_loss: 3.1941 - val_acc: 0.2926\n",
      "Epoch 85/120\n",
      "161892/161892 [==============================] - 313s 2ms/sample - loss: 1.3947 - acc: 0.4924 - val_loss: 3.1981 - val_acc: 0.2930\n",
      "Epoch 86/120\n",
      "161892/161892 [==============================] - 319s 2ms/sample - loss: 1.3948 - acc: 0.4923 - val_loss: 3.1969 - val_acc: 0.2928\n",
      "Epoch 87/120\n",
      "161892/161892 [==============================] - 321s 2ms/sample - loss: 1.3926 - acc: 0.4932 - val_loss: 3.2038 - val_acc: 0.2930\n",
      "Epoch 88/120\n",
      "161892/161892 [==============================] - 317s 2ms/sample - loss: 1.3927 - acc: 0.4927 - val_loss: 3.2009 - val_acc: 0.2944\n",
      "Epoch 89/120\n",
      "161892/161892 [==============================] - 318s 2ms/sample - loss: 1.3920 - acc: 0.4930 - val_loss: 3.2035 - val_acc: 0.2923\n",
      "Epoch 90/120\n",
      "161892/161892 [==============================] - 312s 2ms/sample - loss: 1.3913 - acc: 0.4927 - val_loss: 3.2021 - val_acc: 0.2934\n",
      "Epoch 91/120\n",
      "161892/161892 [==============================] - 317s 2ms/sample - loss: 1.3905 - acc: 0.4933 - val_loss: 3.2065 - val_acc: 0.2913\n",
      "Epoch 92/120\n",
      "161892/161892 [==============================] - 316s 2ms/sample - loss: 1.3901 - acc: 0.4934 - val_loss: 3.2095 - val_acc: 0.2933\n",
      "Epoch 93/120\n",
      "161892/161892 [==============================] - 316s 2ms/sample - loss: 1.3891 - acc: 0.4934 - val_loss: 3.2094 - val_acc: 0.2935\n",
      "Epoch 94/120\n",
      "161892/161892 [==============================] - 317s 2ms/sample - loss: 1.3890 - acc: 0.4938 - val_loss: 3.2171 - val_acc: 0.2920\n",
      "Epoch 95/120\n",
      "161892/161892 [==============================] - 319s 2ms/sample - loss: 1.3887 - acc: 0.4932 - val_loss: 3.2152 - val_acc: 0.2921\n",
      "Epoch 96/120\n",
      "161892/161892 [==============================] - 316s 2ms/sample - loss: 1.3884 - acc: 0.4934 - val_loss: 3.2122 - val_acc: 0.2936\n",
      "Epoch 97/120\n",
      "161892/161892 [==============================] - 314s 2ms/sample - loss: 1.3869 - acc: 0.4937 - val_loss: 3.2209 - val_acc: 0.2905\n",
      "Epoch 98/120\n",
      "161892/161892 [==============================] - 312s 2ms/sample - loss: 1.3872 - acc: 0.4934 - val_loss: 3.2161 - val_acc: 0.2930\n",
      "Epoch 99/120\n",
      "161892/161892 [==============================] - 318s 2ms/sample - loss: 1.3862 - acc: 0.4938 - val_loss: 3.2189 - val_acc: 0.2935\n",
      "Epoch 100/120\n",
      "161892/161892 [==============================] - 325s 2ms/sample - loss: 1.3855 - acc: 0.4939 - val_loss: 3.2169 - val_acc: 0.2920\n",
      "Epoch 101/120\n",
      "161892/161892 [==============================] - 326s 2ms/sample - loss: 1.3859 - acc: 0.4937 - val_loss: 3.2193 - val_acc: 0.2916\n",
      "Epoch 102/120\n",
      "161892/161892 [==============================] - 325s 2ms/sample - loss: 1.3850 - acc: 0.4939 - val_loss: 3.2190 - val_acc: 0.2925\n",
      "Epoch 103/120\n",
      "161892/161892 [==============================] - 321s 2ms/sample - loss: 1.3852 - acc: 0.4938 - val_loss: 3.2224 - val_acc: 0.2934\n",
      "Epoch 104/120\n",
      "161892/161892 [==============================] - 318s 2ms/sample - loss: 1.3830 - acc: 0.4943 - val_loss: 3.2214 - val_acc: 0.2912\n",
      "Epoch 105/120\n",
      "161892/161892 [==============================] - 317s 2ms/sample - loss: 1.3840 - acc: 0.4939 - val_loss: 3.2225 - val_acc: 0.2924\n",
      "Epoch 106/120\n",
      "161892/161892 [==============================] - 326s 2ms/sample - loss: 1.3833 - acc: 0.4941 - val_loss: 3.2280 - val_acc: 0.2906\n",
      "Epoch 107/120\n",
      "161892/161892 [==============================] - 315s 2ms/sample - loss: 1.3797 - acc: 0.4947 - val_loss: 3.2384 - val_acc: 0.2902\n",
      "Epoch 117/120\n",
      "161892/161892 [==============================] - 315s 2ms/sample - loss: 1.3799 - acc: 0.4949 - val_loss: 3.2398 - val_acc: 0.2912\n",
      "Epoch 118/120\n",
      "161892/161892 [==============================] - 317s 2ms/sample - loss: 1.3796 - acc: 0.4946 - val_loss: 3.2425 - val_acc: 0.2915\n",
      "Epoch 119/120\n",
      "161892/161892 [==============================] - 318s 2ms/sample - loss: 1.3800 - acc: 0.4945 - val_loss: 3.2415 - val_acc: 0.2926\n",
      "Epoch 120/120\n",
      "161892/161892 [==============================] - 318s 2ms/sample - loss: 1.3787 - acc: 0.4948 - val_loss: 3.2411 - val_acc: 0.2902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f402015e3c8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    x = {\"encoder_inputs\": encoder_input_data_training, \"decoder_inputs\": decoder_input_data_training}, \n",
    "    y = {'decoder_outputs': decoder_output_data_training}, \n",
    "    batch_size = BATCH_SIZE, \n",
    "    epochs = EPOCHS,\n",
    "    shuffle = True,\n",
    "    validation_split = 0.1\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "6amEIRtjlo-w"
   },
   "outputs": [],
   "source": [
    "model.save(filepath=f\"weight_{EPOCHS}.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMtI7RRq6B_U"
   },
   "source": [
    "# Inference\n",
    "\n",
    "Build the inference model(the same as training model) and load the trained weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "373cZtwERQpm",
    "outputId": "e08200b0-3d87-44b0-d656-f5a3b87e0789"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Weight Loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load the final model\n",
    "model.load_weights('weight_200.h5') \n",
    "print(\"Model Weight Loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "eH4GLMnk60hE"
   },
   "outputs": [],
   "source": [
    "# Function for making inference\n",
    "def make_inference_models():\n",
    "    \n",
    "    # Create a model that takes encoder's input and outputs the states for encoder\n",
    "    encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
    "    \n",
    "    # Create two inputs for decoder which are hidden state (or state h) and cell state (or state c)\n",
    "    decoder_state_input_h = Input(shape = (num_lstm, ), name='decoder_state_input_h')\n",
    "    decoder_state_input_c = Input(shape = (num_lstm, ), name='decoder_state_input_c')\n",
    "    \n",
    "    # Store the two inputs for decoder inside a list\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    # Pass the inputs through LSTM layer you have created before\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state = decoder_states_inputs)\n",
    "    \n",
    "    # Store the outputted hidden state and cell state from LSTM inside a list\n",
    "    decoder_states = [state_h, state_c]\n",
    "\n",
    "    # Pass the output from LSTM layer through the dense layer you have created before\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Create a model that takes decoder_inputs and decoder_states_inputs as inputs and outputs decoder_outputs and decoder_states\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                          [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "7MEWuJdn7H5M"
   },
   "outputs": [],
   "source": [
    "# Function for converting strings to tokens\n",
    "def str_to_tokens(sentence: str, tokenizer, maxlen_questions=TEXT_LIMIT):\n",
    "\n",
    "    # Lowercase the sentence and split it into words\n",
    "    words = sentence.lower().split()\n",
    "\n",
    "    tokens_list = tokenizer.texts_to_sequences([sentence])\n",
    "\n",
    "    # Pad the sequences to be the same length\n",
    "    return pad_sequences(tokens_list , maxlen = maxlen_questions, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "OD5ePfvV63vc"
   },
   "outputs": [],
   "source": [
    "# Initialize the model for inference\n",
    "enc_model , dec_model = make_inference_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tnuR5dmZ66h1",
    "outputId": "70aa47d9-db81-4291-aed6-25883d72bca6"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter question :  how are you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "been better i read about what's been happening with you i\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter question :  how are you doing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not too good\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter question :  what is your name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nulltoken nulltoken and at your service sir\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter question :  what is the weather today\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter question :  what are you saying\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nulltoken another guy came in asking me for a while\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the number of times you want to ask question\n",
    "try:\n",
    "    for _ in range(5):\n",
    "\n",
    "        # Get the input and predict it with the encoder model\n",
    "        encoder_inputs = str_to_tokens(preprocess_text(input('Enter question : ')), tokenizer)\n",
    "        states_values = enc_model.predict(encoder_inputs)\n",
    "\n",
    "        # Initialize the decoder input sequence with the starttoken index\n",
    "        # Reshape this to be a (1, 1) array, since it is a sequence of 1 sample for 1 timestep\n",
    "        empty_target_seq = np.zeros((1, 1))\n",
    "\n",
    "        # Update the target sequence with index of \"start\"\n",
    "        empty_target_seq[0, 0] = tokenizer.word_index[\"starttoken\"]\n",
    "        # Initialize the stop condition with False\n",
    "        stop_condition = False\n",
    "\n",
    "        # Initialize the decoded words with an empty string\n",
    "        decoded_translation = []\n",
    "\n",
    "        # While stop_condition is false\n",
    "        while not stop_condition :\n",
    "\n",
    "            # Predict the (target sequence + the output from encoder model) with decoder model\n",
    "            dec_outputs , h , c = dec_model.predict([empty_target_seq] + states_values)\n",
    "            # Get the index for sampled word using the dec_outputs\n",
    "            # dec_outputs is a numpy array of the shape (sample, timesteps, VOCAB_SIZE)\n",
    "            # To start, we can just pick the word with the higest probability - greedy search\n",
    "            sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
    "            \n",
    "            # Initialize the sampled word with None\n",
    "            sampled_word = None\n",
    "\n",
    "            # Iterate through words and their indexes\n",
    "            for word, index in tokenizer.word_index.items() :\n",
    "\n",
    "                # If the index is equal to sampled word's index\n",
    "                if sampled_word_index == index :\n",
    "\n",
    "                    # Add the word to the decoded string\n",
    "                    decoded_translation.append(word)\n",
    "\n",
    "                    # Update the sampled word\n",
    "                    sampled_word = word\n",
    "\n",
    "            # If sampled word is equal to \"end\" OR the length of decoded string is more that what is allowed\n",
    "            if sampled_word == 'endtoken' or len(decoded_translation) > TEXT_LIMIT:\n",
    "\n",
    "                # Make the stop_condition to true\n",
    "                stop_condition = True\n",
    "\n",
    "            # Initialize back the target sequence to zero - array([[0.]])    \n",
    "            empty_target_seq = np.zeros(shape = (1, 1))  \n",
    "\n",
    "            # Update the target sequence with index of \"start\"\n",
    "            empty_target_seq[0, 0] = sampled_word_index\n",
    "\n",
    "            # Get the state values\n",
    "            states_values = [h, c] \n",
    "\n",
    "            # Print the decoded string\n",
    "        print(' '.join(decoded_translation[:-1]))\n",
    "except KeyboardInterrupt:\n",
    "    print('Ending conversational agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqeHbygA8108"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CIS519_Finial_Project.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
