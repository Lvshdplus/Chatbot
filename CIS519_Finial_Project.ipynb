{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLEFunpdhlzL"
   },
   "source": [
    "# 1. Data Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zw0TQGqWh_cY"
   },
   "source": [
    "## 1.1 Data Collection \n",
    "\n",
    "Download the datasets from [Cornell Movie Datasets Website](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) and unzip the data into txt files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hQ08H1wEiHzD",
    "outputId": "446f43b5-e33e-4a72-8fdc-a01053dfa42e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-19 19:23:37--  http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
      "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
      "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9916637 (9.5M) [application/zip]\n",
      "Saving to: ‘cornell_movie_dialogs_corpus.zip’\n",
      "\n",
      "cornell_movie_dialo 100%[===================>]   9.46M  58.6MB/s    in 0.2s    \n",
      "\n",
      "2022-04-19 19:23:37 (58.6 MB/s) - ‘cornell_movie_dialogs_corpus.zip’ saved [9916637/9916637]\n",
      "\n",
      "Archive:  cornell_movie_dialogs_corpus.zip\n",
      "   creating: cornell movie-dialogs corpus/\n",
      "  inflating: cornell movie-dialogs corpus/.DS_Store  \n",
      "   creating: __MACOSX/\n",
      "   creating: __MACOSX/cornell movie-dialogs corpus/\n",
      "  inflating: __MACOSX/cornell movie-dialogs corpus/._.DS_Store  \n",
      "  inflating: cornell movie-dialogs corpus/chameleons.pdf  \n",
      "  inflating: __MACOSX/cornell movie-dialogs corpus/._chameleons.pdf  \n",
      "  inflating: cornell movie-dialogs corpus/movie_characters_metadata.txt  \n",
      "  inflating: cornell movie-dialogs corpus/movie_conversations.txt  \n",
      "  inflating: cornell movie-dialogs corpus/movie_lines.txt  \n",
      "  inflating: cornell movie-dialogs corpus/movie_titles_metadata.txt  \n",
      "  inflating: cornell movie-dialogs corpus/raw_script_urls.txt  \n",
      "  inflating: cornell movie-dialogs corpus/README.txt  \n",
      "  inflating: __MACOSX/cornell movie-dialogs corpus/._README.txt  \n"
     ]
    }
   ],
   "source": [
    "! wget -nc \"http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\"\n",
    "! unzip cornell_movie_dialogs_corpus.zip\n",
    "! rm cornell_movie_dialogs_corpus.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwM5YMC1QXH_"
   },
   "source": [
    "## 1.2 Data Cleaning & Wrangling\n",
    "\n",
    "Clean the data and convert it into the form of a dialog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Bbp11GwnmtYG"
   },
   "outputs": [],
   "source": [
    "# open dialog files\n",
    "movie_lines = open('cornell movie-dialogs corpus/movie_lines.txt', encoding='utf-8',errors='ignore').read().split('\\n')\n",
    "movie_conversations = open('cornell movie-dialogs corpus/movie_conversations.txt', encoding='utf-8',errors='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yaSnbTgHnnwH"
   },
   "outputs": [],
   "source": [
    "# build a dictionary to record (line_number, dialog) mappings\n",
    "line_to_dialog = {}\n",
    "for line in movie_lines:\n",
    "  line_splited = line.split(' +++$+++ ')\n",
    "  line_to_dialog[line_splited[0]] = line_splited[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ktu9maMdp5YE"
   },
   "outputs": [],
   "source": [
    "# build dialog fragments\n",
    "dialog_fragments = []\n",
    "for conversation in movie_conversations:\n",
    "  ## convert u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197'] to 'L194', 'L195', 'L196', 'L197'\n",
    "  dialog_instance = conversation.split(' +++$+++ ')[-1][1:-1]\n",
    "  ## convert 'L194', 'L195', 'L196', 'L197' to ['L194', 'L195', 'L196', 'L197']\n",
    "  dialog_fragments.append(dialog_instance.replace(\"'\", \" \").replace(\",\",\"\").split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "n6ZyUbPJk6xE"
   },
   "outputs": [],
   "source": [
    "# convert dialog fragments into (question, answer) pairs\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for frag in dialog_fragments:\n",
    "    for i in range(1, len(frag)):\n",
    "        questions.append(line_to_dialog[frag[i-1]])\n",
    "        answers.append(line_to_dialog[frag[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sAWPaVbtmdX4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "\n",
      "Dialog A: Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "\n",
      "Dialog B: Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "Dialog A: Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "\n",
      "Dialog B: Not the hacking and gagging and spitting part.  Please.\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "Dialog A: Not the hacking and gagging and spitting part.  Please.\n",
      "\n",
      "Dialog B: Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "Dialog A: You're asking me out.  That's so cute. What's your name again?\n",
      "\n",
      "Dialog B: Forget it.\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "Dialog A: No, no, it's my fault -- we didn't have a proper introduction ---\n",
      "\n",
      "Dialog B: Cameron.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show questions and answers pairs\n",
    "def qa_show(num):\n",
    "  for i in range(num):\n",
    "    print('-------------------------------------------------\\n')\n",
    "    print(f\"Dialog A: {questions[i]}\\n\")\n",
    "    print(f\"Dialog B: {answers[i]}\\n\")\n",
    "\n",
    "qa_show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixNAnkQiQ6wy"
   },
   "source": [
    "## 1.3 Text Preprocessing\n",
    "\n",
    "Preprocess the texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XkeekSzCSEfn"
   },
   "outputs": [],
   "source": [
    "# Global variables\n",
    "TEXT_LIMIT = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9WZ0gkXaSNNK"
   },
   "outputs": [],
   "source": [
    "# filter out long dialogs\n",
    "def filter_long_texts(questions, answers, limit):\n",
    "    short_questions = []\n",
    "    short_answers = []\n",
    "    for i in range(len(questions)):\n",
    "        # if len(questions[i]) <= TEXT_LIMIT and len(answers[i]) <=TEXT_LIMIT:\n",
    "        if len(questions[i].split()) <= TEXT_LIMIT:\n",
    "            short_questions.append(questions[i])\n",
    "            short_answers.append(answers[i])\n",
    "    return short_questions, short_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Ix7oU7nabaun"
   },
   "outputs": [],
   "source": [
    "filtered_questions, filtered_answers = filter_long_texts(questions, answers, TEXT_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pY6jluMCSsn8"
   },
   "outputs": [],
   "source": [
    "# clean the texts\n",
    "import re\n",
    "\n",
    "replacement_patterns = [\n",
    "  (r'won\\'t', 'will not'),\n",
    "  (r'can\\'t', 'cannot'),\n",
    "  (r'i\\'m', 'i am'),\n",
    "  (r'ain\\'t', 'is not'),\n",
    "  (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "  (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "  (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "  (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "  (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "  (r'(\\w+)\\'d', '\\g<1> would'),\n",
    "]\n",
    "\n",
    "class TextCleaner(object):\n",
    "  def __init__(self, patterns=replacement_patterns):\n",
    "    self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "    \n",
    "  def replace(self, text):\n",
    "    s = text\n",
    "    for (pattern, replace) in self.patterns:\n",
    "      s = re.sub(pattern, replace, s)\n",
    "    return s\n",
    "\n",
    "cleaner = TextCleaner()\n",
    "\n",
    "# Function for preprocessing the given text\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Decontracting the text (e.g. it's -> it is)\n",
    "    text = cleaner.replace(text)\n",
    "    \n",
    "    # Remove the punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r\"[ ]+\", \" \", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XcPRHsfIXDtc",
    "outputId": "2a4561f0-d095-4834-c021-f9fbbb1cbfb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "\n",
      "Dialog A: well i thought we would start with pronunciation if that is okay with you \n",
      "\n",
      "Dialog B: Not the hacking and gagging and spitting part. Please.\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "Dialog A: not the hacking and gagging and spitting part please \n",
      "\n",
      "Dialog B: Okay... then how 'bout we try out some French cuisine. Saturday?\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "Dialog A: you are asking me out that is so cute what is your name again \n",
      "\n",
      "Dialog B: Forget it.\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "Dialog A: no no it is my fault we did not have a proper introduction \n",
      "\n",
      "Dialog B: Cameron.\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "Dialog A: cameron \n",
      "\n",
      "Dialog B: The thing is, Cameron -- I'm at the mercy of a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_questions = [preprocess_text(que) for que in filtered_questions]\n",
    "cleaned_answers = [' '.join(ans.split()[:TEXT_LIMIT-2]) for ans in filtered_answers]\n",
    "\n",
    "del(questions, answers, filtered_questions, filtered_answers)\n",
    "\n",
    "for i in range(5):\n",
    "    print('-------------------------------------------------\\n')\n",
    "    print(f'Dialog A: {cleaned_questions[i]}\\n')\n",
    "    print(f'Dialog B: {cleaned_answers[i]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEuvFeGuF9bq"
   },
   "source": [
    "After preprocessing the dataset, we should add a start tag (e.g. `<start>`) and an end tag (e.g. `<end>`) to answers. Remember that we will only add these tags to answers and not questions.\n",
    "\n",
    "When Seq2Seq model is generating the word answers, we can first send it the `<start>` to begin the word generation. When `<end>` is generated, we will stop the iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ETc_LE9sXX0O"
   },
   "outputs": [],
   "source": [
    "cleaned_answers = [\"starttoken \" + ans + \" endtoken\" for ans in cleaned_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lakxrfs9H0uQ"
   },
   "outputs": [],
   "source": [
    "# trim the data in case of running out of RAM\n",
    "cleaned_questions = cleaned_questions[:30000]\n",
    "cleaned_answers = cleaned_answers[:30000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pj2VSg6ento0"
   },
   "source": [
    "## 1.4 Input Encoding\n",
    "\n",
    "Convet String input into numerical values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6B4lo51bH6Fs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import preprocessing, utils\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "CJUf6CjJ37u7"
   },
   "outputs": [],
   "source": [
    "NUM_WORDS = 3500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TjSxe5uGIH6q",
    "outputId": "ed256e97-3f6c-407f-f305-bebc54a9616d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE : 14987\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = preprocessing.text.Tokenizer(num_words = NUM_WORDS, oov_token='nulltoken')\n",
    "\n",
    "# Fit the tokenizer to questions and answers\n",
    "tokenizer.fit_on_texts(cleaned_questions + cleaned_answers)\n",
    "\n",
    "# Get the total vocab size\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "# display(tokenizer.word_index)\n",
    "print( 'VOCAB SIZE : {}'.format(VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XGTAwFiL9Wq3",
    "outputId": "d45d2a0c-0397-4ef5-9958-2ca5bda07bdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index[\"starttoken\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aCeEQiMwIZas",
    "outputId": "dd38922f-dd4d-4f57-c6d7-2e94f4e62d4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 13) 13\n"
     ]
    }
   ],
   "source": [
    "### encoder input data\n",
    "\n",
    "# Tokenize the questions\n",
    "tokenized_questions = tokenizer.texts_to_sequences(cleaned_questions)\n",
    "\n",
    "# Get the length of longest sequence\n",
    "# maxlen_questions = max([len(x) for x in tokenized_questions])\n",
    "\n",
    "# Pad the sequences\n",
    "padded_questions = pad_sequences(tokenized_questions, maxlen=TEXT_LIMIT, padding='post')\n",
    "\n",
    "# Convert the sequences into array\n",
    "encoder_input_data = np.array(padded_questions)\n",
    "# print(encoder_input_data[0])\n",
    "print(encoder_input_data.shape, TEXT_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uoKg2PHoIkcH",
    "outputId": "092abd9d-dba9-4bc6-e23e-ec6a01e7f418"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 13) 13\n"
     ]
    }
   ],
   "source": [
    "### decoder input data\n",
    "\n",
    "# Tokenize the answers\n",
    "tokenized_answers = tokenizer.texts_to_sequences(cleaned_answers)\n",
    "\n",
    "# Get the length of longest sequence\n",
    "# maxlen_answers = max([len(x) for x in tokenized_answers])\n",
    "\n",
    "# Pad the sequences\n",
    "padded_answers = pad_sequences(tokenized_answers, maxlen=TEXT_LIMIT, padding='post')\n",
    "\n",
    "# Convert the sequences into array\n",
    "decoder_input_data = np.array(padded_answers)\n",
    "\n",
    "print(decoder_input_data.shape, TEXT_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "SXi3CTOMI8AX"
   },
   "outputs": [],
   "source": [
    "### decoder_output_data\n",
    "\n",
    "# Iterate through index of tokenized answers\n",
    "for i in range(len(tokenized_answers)):\n",
    "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "\n",
    "# Pad the tokenized answers\n",
    "padded_answers = pad_sequences(tokenized_answers, maxlen = TEXT_LIMIT, padding = 'post')\n",
    "\n",
    "# One hot encode\n",
    "onehot_answers = utils.to_categorical(padded_answers, NUM_WORDS+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZnyhJ7VC4-YY",
    "outputId": "03520b6b-3622-4ef4-c6b8-8c636972f595"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 13, 3501)\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy array\n",
    "decoder_output_data = np.array(onehot_answers)\n",
    "# print(decoder_output_data[0])\n",
    "print(decoder_output_data.shape)\n",
    "\n",
    "del (onehot_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ZmOuXZ6vJJcj"
   },
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  ['/job:localhost/replica:0/task:0/device:CPU:0', '/job:localhost/replica:0/task:0/device:XLA_CPU:0', '/job:localhost/replica:0/task:0/device:XLA_GPU:0', '/job:localhost/replica:0/task:0/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", tf.config.experimental_list_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "SMch0EsnkmMN"
   },
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200\n",
    "VOCAB_SIZE = NUM_WORDS + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Wwi7M2Nmk1oR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "### Encoder Input\n",
    "embed_dim = 50\n",
    "num_lstm = 400\n",
    "\n",
    "# Input for encoder\n",
    "encoder_inputs = Input(shape = (None, ), name='encoder_inputs')\n",
    "\n",
    "# Embedding layer\n",
    "# Why mask_zero = True? https://www.tensorflow.org/guide/keras/masking_and_padding\n",
    "encoder_embedding = Embedding(input_dim = VOCAB_SIZE, output_dim = embed_dim, mask_zero = True, name='encoder_embedding')(encoder_inputs)\n",
    "\n",
    "# LSTM layer (that returns states in addition to output)\n",
    "encoder_outputs, state_h, state_c = LSTM(units = num_lstm, return_state = True, name='encoder_lstm')(encoder_embedding)\n",
    "\n",
    "# Get the states for encoder\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "wbyKdEcXk6d6"
   },
   "outputs": [],
   "source": [
    "### Decoder\n",
    "\n",
    "# Input for decoder\n",
    "decoder_inputs = Input(shape = (None,  ), name='decoder_inputs')\n",
    "\n",
    "# Embedding layer\n",
    "decoder_embedding = Embedding(input_dim = VOCAB_SIZE, output_dim = embed_dim , mask_zero = True, name='decoder_embedding')(decoder_inputs)\n",
    "\n",
    "# LSTM layer (that returns states and sequences as well)\n",
    "decoder_lstm = LSTM(units = num_lstm , return_state = True , return_sequences = True, name='decoder_lstm')\n",
    "\n",
    "# Get the output of LSTM layer, using the initial states from the encoder\n",
    "decoder_outputs, _, _ = decoder_lstm(inputs = decoder_embedding, initial_state = encoder_states)\n",
    "\n",
    "# Dense layer\n",
    "decoder_dense = Dense(units = VOCAB_SIZE, activation = softmax, name='decoder_outputs') \n",
    "\n",
    "# Get the output of Dense layer\n",
    "output = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "xK6yhoCtlDYK"
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Model([encoder_inputs, decoder_inputs], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "T_nMCQ3VlHoO"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', metrics=['acc'], loss = \"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bHJ7aT6GlH4x",
    "outputId": "b5d49285-e393-4cc8-dc20-b70ea2d291f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedding (Embedding)   (None, None, 50)     175050      encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedding (Embedding)   (None, None, 50)     175050      decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm (LSTM)             [(None, 400), (None, 721600      encoder_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 400),  721600      decoder_embedding[0][0]          \n",
      "                                                                 encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_outputs (Dense)         (None, None, 3501)   1403901     decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 3,197,201\n",
      "Trainable params: 3,197,201\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IfVO8dDhlNeH",
    "outputId": "b5f08152-316c-4bf4-e01d-7dd2be23ff72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:163: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:189: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "[2022-04-19 20:41:44.799 ip-172-16-3-167:4839 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2022-04-19 20:41:44.825 ip-172-16-3-167:4839 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "Train on 30000 samples\n",
      "Epoch 1/200\n",
      "30000/30000 [==============================] - 42s 1ms/sample - loss: 3.2326 - acc: 0.2397\n",
      "Epoch 2/200\n",
      "30000/30000 [==============================] - 41s 1ms/sample - loss: 2.9283 - acc: 0.2764\n",
      "Epoch 3/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 2.7753 - acc: 0.2957\n",
      "Epoch 4/200\n",
      "30000/30000 [==============================] - 41s 1ms/sample - loss: 2.6689 - acc: 0.3066\n",
      "Epoch 5/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 2.5826 - acc: 0.3147\n",
      "Epoch 6/200\n",
      "30000/30000 [==============================] - 41s 1ms/sample - loss: 2.5071 - acc: 0.3214\n",
      "Epoch 7/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 2.4365 - acc: 0.3284\n",
      "Epoch 8/200\n",
      "30000/30000 [==============================] - 41s 1ms/sample - loss: 2.3688 - acc: 0.3344\n",
      "Epoch 9/200\n",
      "30000/30000 [==============================] - 41s 1ms/sample - loss: 2.3016 - acc: 0.3406\n",
      "Epoch 10/200\n",
      "30000/30000 [==============================] - 41s 1ms/sample - loss: 2.2365 - acc: 0.3474\n",
      "Epoch 11/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 2.1704 - acc: 0.3557\n",
      "Epoch 12/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 2.1068 - acc: 0.3649\n",
      "Epoch 13/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 2.0436 - acc: 0.3753\n",
      "Epoch 14/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 1.9834 - acc: 0.3856\n",
      "Epoch 15/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 1.9238 - acc: 0.3973\n",
      "Epoch 16/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 1.8657 - acc: 0.4093\n",
      "Epoch 17/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 1.8088 - acc: 0.4221\n",
      "Epoch 18/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 1.7533 - acc: 0.4346\n",
      "Epoch 19/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 1.6987 - acc: 0.4479\n",
      "Epoch 20/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 1.6459 - acc: 0.4613\n",
      "Epoch 21/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 1.5941 - acc: 0.4751\n",
      "Epoch 22/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 1.5428 - acc: 0.4886\n",
      "Epoch 23/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 1.4939 - acc: 0.5021\n",
      "Epoch 24/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 1.4453 - acc: 0.5164\n",
      "Epoch 25/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 1.3987 - acc: 0.5302\n",
      "Epoch 26/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 1.3523 - acc: 0.5446\n",
      "Epoch 27/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 1.3080 - acc: 0.5584\n",
      "Epoch 28/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 1.2660 - acc: 0.5711\n",
      "Epoch 29/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 1.2230 - acc: 0.5846\n",
      "Epoch 30/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 1.1827 - acc: 0.5970\n",
      "Epoch 31/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 1.1428 - acc: 0.6099\n",
      "Epoch 32/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 1.1053 - acc: 0.6218\n",
      "Epoch 33/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 1.0697 - acc: 0.6337\n",
      "Epoch 34/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 1.0321 - acc: 0.6471\n",
      "Epoch 35/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.9980 - acc: 0.6579\n",
      "Epoch 36/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.9649 - acc: 0.6698\n",
      "Epoch 37/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.9331 - acc: 0.6799\n",
      "Epoch 38/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.9033 - acc: 0.6904\n",
      "Epoch 39/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.8740 - acc: 0.7001\n",
      "Epoch 40/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.8446 - acc: 0.7101\n",
      "Epoch 41/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.8176 - acc: 0.7191\n",
      "Epoch 42/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.7916 - acc: 0.7287\n",
      "Epoch 43/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.7670 - acc: 0.7366\n",
      "Epoch 44/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.7423 - acc: 0.7458\n",
      "Epoch 45/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.7207 - acc: 0.7527\n",
      "Epoch 46/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.6962 - acc: 0.7619\n",
      "Epoch 47/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.6758 - acc: 0.7683\n",
      "Epoch 48/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.6561 - acc: 0.7760\n",
      "Epoch 49/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.6355 - acc: 0.7825\n",
      "Epoch 50/200\n",
      "30000/30000 [==============================] - 41s 1ms/sample - loss: 0.6164 - acc: 0.7891\n",
      "Epoch 51/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.5986 - acc: 0.7952\n",
      "Epoch 52/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.5832 - acc: 0.8003\n",
      "Epoch 53/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.5667 - acc: 0.8060\n",
      "Epoch 54/200\n",
      "30000/30000 [==============================] - 41s 1ms/sample - loss: 0.5479 - acc: 0.8130\n",
      "Epoch 55/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.5340 - acc: 0.8172\n",
      "Epoch 56/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.5196 - acc: 0.8220\n",
      "Epoch 57/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.5083 - acc: 0.8252\n",
      "Epoch 58/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.4938 - acc: 0.8314\n",
      "Epoch 59/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.4780 - acc: 0.8370\n",
      "Epoch 60/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.4669 - acc: 0.8403\n",
      "Epoch 61/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.4590 - acc: 0.8426\n",
      "Epoch 62/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.4472 - acc: 0.8467\n",
      "Epoch 63/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.4364 - acc: 0.8499\n",
      "Epoch 64/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.4229 - acc: 0.8550\n",
      "Epoch 65/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.4163 - acc: 0.8571\n",
      "Epoch 66/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.4041 - acc: 0.8617\n",
      "Epoch 67/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.3981 - acc: 0.8634\n",
      "Epoch 68/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.3850 - acc: 0.8685\n",
      "Epoch 69/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.3837 - acc: 0.8670\n",
      "Epoch 70/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.3736 - acc: 0.8710\n",
      "Epoch 71/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.3634 - acc: 0.8750\n",
      "Epoch 72/200\n",
      "30000/30000 [==============================] - 41s 1ms/sample - loss: 0.3557 - acc: 0.8780\n",
      "Epoch 73/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.3512 - acc: 0.8788\n",
      "Epoch 74/200\n",
      "30000/30000 [==============================] - 41s 1ms/sample - loss: 0.3451 - acc: 0.8808\n",
      "Epoch 75/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.3380 - acc: 0.8830\n",
      "Epoch 76/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.3284 - acc: 0.8863\n",
      "Epoch 77/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.3273 - acc: 0.8857\n",
      "Epoch 78/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.3201 - acc: 0.8885\n",
      "Epoch 79/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.3127 - acc: 0.8918\n",
      "Epoch 80/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.3070 - acc: 0.8934\n",
      "Epoch 81/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.3056 - acc: 0.8932\n",
      "Epoch 82/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.2995 - acc: 0.8949\n",
      "Epoch 83/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.2973 - acc: 0.8951\n",
      "Epoch 84/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.2883 - acc: 0.8993\n",
      "Epoch 85/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.2896 - acc: 0.8976\n",
      "Epoch 86/200\n",
      "30000/30000 [==============================] - 41s 1ms/sample - loss: 0.2796 - acc: 0.9017\n",
      "Epoch 87/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.2800 - acc: 0.9005\n",
      "Epoch 88/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.2746 - acc: 0.9029\n",
      "Epoch 89/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.2723 - acc: 0.9032\n",
      "Epoch 90/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.2668 - acc: 0.9052\n",
      "Epoch 91/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.2594 - acc: 0.9085\n",
      "Epoch 92/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.2611 - acc: 0.9061\n",
      "Epoch 93/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.2639 - acc: 0.9045\n",
      "Epoch 94/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.2528 - acc: 0.9095\n",
      "Epoch 95/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.2474 - acc: 0.9120\n",
      "Epoch 96/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.2459 - acc: 0.9123\n",
      "Epoch 97/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.2459 - acc: 0.9114\n",
      "Epoch 98/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.2496 - acc: 0.9090\n",
      "Epoch 99/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.2426 - acc: 0.9117\n",
      "Epoch 100/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.2373 - acc: 0.9145\n",
      "Epoch 101/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.2346 - acc: 0.9146\n",
      "Epoch 102/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.2269 - acc: 0.9189\n",
      "Epoch 103/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.2331 - acc: 0.9153\n",
      "Epoch 104/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.2300 - acc: 0.9154\n",
      "Epoch 105/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.2285 - acc: 0.9166\n",
      "Epoch 106/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.2305 - acc: 0.9151\n",
      "Epoch 107/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.2226 - acc: 0.9185\n",
      "Epoch 108/200\n",
      "30000/30000 [==============================] - 41s 1ms/sample - loss: 0.2199 - acc: 0.9189\n",
      "Epoch 109/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.2210 - acc: 0.9188\n",
      "Epoch 110/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.2151 - acc: 0.9210\n",
      "Epoch 111/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.2111 - acc: 0.9231\n",
      "Epoch 112/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.2105 - acc: 0.9226\n",
      "Epoch 113/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.2145 - acc: 0.9204\n",
      "Epoch 114/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.2130 - acc: 0.9204\n",
      "Epoch 115/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.2139 - acc: 0.9196\n",
      "Epoch 116/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.2145 - acc: 0.9187\n",
      "Epoch 117/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1995 - acc: 0.9265\n",
      "Epoch 118/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.2079 - acc: 0.9219\n",
      "Epoch 119/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.2076 - acc: 0.9219\n",
      "Epoch 120/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.2095 - acc: 0.9206\n",
      "Epoch 121/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.1970 - acc: 0.9268\n",
      "Epoch 122/200\n",
      "30000/30000 [==============================] - 41s 1ms/sample - loss: 0.1901 - acc: 0.9300\n",
      "Epoch 123/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1971 - acc: 0.9259\n",
      "Epoch 124/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.2038 - acc: 0.9221\n",
      "Epoch 125/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1935 - acc: 0.9272\n",
      "Epoch 126/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1933 - acc: 0.9275\n",
      "Epoch 127/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1990 - acc: 0.9238\n",
      "Epoch 128/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1913 - acc: 0.9275\n",
      "Epoch 129/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.2068 - acc: 0.9198\n",
      "Epoch 130/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1874 - acc: 0.9289\n",
      "Epoch 131/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1831 - acc: 0.9311\n",
      "Epoch 132/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1916 - acc: 0.9264\n",
      "Epoch 133/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.1887 - acc: 0.9281\n",
      "Epoch 134/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1923 - acc: 0.9255\n",
      "Epoch 135/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1843 - acc: 0.9298\n",
      "Epoch 136/200\n",
      "30000/30000 [==============================] - 41s 1ms/sample - loss: 0.1831 - acc: 0.9308\n",
      "Epoch 137/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1793 - acc: 0.9320\n",
      "Epoch 138/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1860 - acc: 0.9281\n",
      "Epoch 139/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.1849 - acc: 0.9284\n",
      "Epoch 140/200\n",
      "30000/30000 [==============================] - 41s 1ms/sample - loss: 0.1847 - acc: 0.9283\n",
      "Epoch 141/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1803 - acc: 0.9301\n",
      "Epoch 142/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1795 - acc: 0.9306\n",
      "Epoch 143/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.1758 - acc: 0.9326\n",
      "Epoch 144/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1790 - acc: 0.9312\n",
      "Epoch 145/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1850 - acc: 0.9273\n",
      "Epoch 146/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1785 - acc: 0.9304\n",
      "Epoch 147/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.1780 - acc: 0.9305\n",
      "Epoch 148/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1744 - acc: 0.9317\n",
      "Epoch 149/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1731 - acc: 0.9329\n",
      "Epoch 150/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1722 - acc: 0.9332\n",
      "Epoch 151/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.1737 - acc: 0.9323\n",
      "Epoch 152/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1830 - acc: 0.9270\n",
      "Epoch 153/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1735 - acc: 0.9323\n",
      "Epoch 154/200\n",
      "30000/30000 [==============================] - 41s 1ms/sample - loss: 0.1750 - acc: 0.9311\n",
      "Epoch 155/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.1756 - acc: 0.9309\n",
      "Epoch 156/200\n",
      "30000/30000 [==============================] - 41s 1ms/sample - loss: 0.1721 - acc: 0.9329\n",
      "Epoch 157/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.1645 - acc: 0.9362\n",
      "Epoch 158/200\n",
      "30000/30000 [==============================] - 41s 1ms/sample - loss: 0.1680 - acc: 0.9343\n",
      "Epoch 159/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.1748 - acc: 0.9303\n",
      "Epoch 160/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1736 - acc: 0.9308\n",
      "Epoch 161/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.1746 - acc: 0.9304\n",
      "Epoch 162/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1666 - acc: 0.9340\n",
      "Epoch 163/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1638 - acc: 0.9358\n",
      "Epoch 164/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1656 - acc: 0.9343\n",
      "Epoch 165/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.1701 - acc: 0.9319\n",
      "Epoch 166/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1615 - acc: 0.9365\n",
      "Epoch 167/200\n",
      "30000/30000 [==============================] - 41s 1ms/sample - loss: 0.1656 - acc: 0.9346\n",
      "Epoch 168/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1704 - acc: 0.9318\n",
      "Epoch 169/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.1756 - acc: 0.9291\n",
      "Epoch 170/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1601 - acc: 0.9366\n",
      "Epoch 171/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.1555 - acc: 0.9389\n",
      "Epoch 172/200\n",
      "30000/30000 [==============================] - 41s 1ms/sample - loss: 0.1667 - acc: 0.9334\n",
      "Epoch 173/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.1664 - acc: 0.9332\n",
      "Epoch 174/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1700 - acc: 0.9311\n",
      "Epoch 175/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.1691 - acc: 0.9321\n",
      "Epoch 176/200\n",
      "30000/30000 [==============================] - 41s 1ms/sample - loss: 0.1553 - acc: 0.9385\n",
      "Epoch 177/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.1661 - acc: 0.9330\n",
      "Epoch 178/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1573 - acc: 0.9374\n",
      "Epoch 179/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.1628 - acc: 0.9341\n",
      "Epoch 180/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1623 - acc: 0.9347\n",
      "Epoch 181/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1635 - acc: 0.9342\n",
      "Epoch 182/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1629 - acc: 0.9343\n",
      "Epoch 183/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.1621 - acc: 0.9345\n",
      "Epoch 184/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1672 - acc: 0.9316\n",
      "Epoch 185/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1576 - acc: 0.9369\n",
      "Epoch 186/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1547 - acc: 0.9380\n",
      "Epoch 187/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1593 - acc: 0.9361\n",
      "Epoch 188/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1601 - acc: 0.9352\n",
      "Epoch 189/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.1570 - acc: 0.9365\n",
      "Epoch 190/200\n",
      "30000/30000 [==============================] - 41s 1ms/sample - loss: 0.1604 - acc: 0.9351\n",
      "Epoch 191/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.1668 - acc: 0.9317\n",
      "Epoch 192/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1636 - acc: 0.9333\n",
      "Epoch 193/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.1522 - acc: 0.9386\n",
      "Epoch 194/200\n",
      "30000/30000 [==============================] - 41s 1ms/sample - loss: 0.1554 - acc: 0.9370\n",
      "Epoch 195/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.1536 - acc: 0.9383\n",
      "Epoch 196/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1505 - acc: 0.9394\n",
      "Epoch 197/200\n",
      "30000/30000 [==============================] - 39s 1ms/sample - loss: 0.1574 - acc: 0.9359\n",
      "Epoch 198/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1664 - acc: 0.9309\n",
      "Epoch 199/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1601 - acc: 0.9347\n",
      "Epoch 200/200\n",
      "30000/30000 [==============================] - 40s 1ms/sample - loss: 0.1493 - acc: 0.9401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff1b9c98320>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    x = {\"encoder_inputs\": encoder_input_data, \"decoder_inputs\": decoder_input_data}, \n",
    "    y = {'decoder_outputs': decoder_output_data}, \n",
    "    batch_size = BATCH_SIZE, \n",
    "    epochs = EPOCHS\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "6amEIRtjlo-w"
   },
   "outputs": [],
   "source": [
    "model.save(filepath=f\"weight_{EPOCHS}.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMtI7RRq6B_U"
   },
   "source": [
    "# Inference\n",
    "\n",
    "Build the inference model(the same as training model) and load the trained weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "373cZtwERQpm",
    "outputId": "e08200b0-3d87-44b0-d656-f5a3b87e0789"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Weight Loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load the final model\n",
    "model.load_weights('weight_200.h5') \n",
    "print(\"Model Weight Loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "eH4GLMnk60hE"
   },
   "outputs": [],
   "source": [
    "# Function for making inference\n",
    "def make_inference_models():\n",
    "    \n",
    "    # Create a model that takes encoder's input and outputs the states for encoder\n",
    "    encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
    "    \n",
    "    # Create two inputs for decoder which are hidden state (or state h) and cell state (or state c)\n",
    "    decoder_state_input_h = Input(shape = (num_lstm, ), name='decoder_state_input_h')\n",
    "    decoder_state_input_c = Input(shape = (num_lstm, ), name='decoder_state_input_c')\n",
    "    \n",
    "    # Store the two inputs for decoder inside a list\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    # Pass the inputs through LSTM layer you have created before\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state = decoder_states_inputs)\n",
    "    \n",
    "    # Store the outputted hidden state and cell state from LSTM inside a list\n",
    "    decoder_states = [state_h, state_c]\n",
    "\n",
    "    # Pass the output from LSTM layer through the dense layer you have created before\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Create a model that takes decoder_inputs and decoder_states_inputs as inputs and outputs decoder_outputs and decoder_states\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                          [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "7MEWuJdn7H5M"
   },
   "outputs": [],
   "source": [
    "# Function for converting strings to tokens\n",
    "def str_to_tokens(sentence: str, tokenizer, maxlen_questions=TEXT_LIMIT):\n",
    "\n",
    "    # Lowercase the sentence and split it into words\n",
    "    words = sentence.lower().split()\n",
    "\n",
    "    tokens_list = tokenizer.texts_to_sequences([sentence])\n",
    "\n",
    "    # Pad the sequences to be the same length\n",
    "    return pad_sequences(tokens_list , maxlen = maxlen_questions, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "OD5ePfvV63vc"
   },
   "outputs": [],
   "source": [
    "# Initialize the model for inference\n",
    "enc_model , dec_model = make_inference_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tnuR5dmZ66h1",
    "outputId": "70aa47d9-db81-4291-aed6-25883d72bca6"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter question :  how are you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "been better i read about what's been happening with you i\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter question :  how are you doing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not too good\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter question :  what is your name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nulltoken nulltoken and at your service sir\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter question :  what is the weather today\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter question :  what are you saying\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nulltoken another guy came in asking me for a while\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the number of times you want to ask question\n",
    "try:\n",
    "    for _ in range(5):\n",
    "\n",
    "        # Get the input and predict it with the encoder model\n",
    "        encoder_inputs = str_to_tokens(preprocess_text(input('Enter question : ')), tokenizer)\n",
    "        states_values = enc_model.predict(encoder_inputs)\n",
    "\n",
    "        # Initialize the decoder input sequence with the starttoken index\n",
    "        # Reshape this to be a (1, 1) array, since it is a sequence of 1 sample for 1 timestep\n",
    "        empty_target_seq = np.zeros((1, 1))\n",
    "\n",
    "        # Update the target sequence with index of \"start\"\n",
    "        empty_target_seq[0, 0] = tokenizer.word_index[\"starttoken\"]\n",
    "        # Initialize the stop condition with False\n",
    "        stop_condition = False\n",
    "\n",
    "        # Initialize the decoded words with an empty string\n",
    "        decoded_translation = []\n",
    "\n",
    "        # While stop_condition is false\n",
    "        while not stop_condition :\n",
    "\n",
    "            # Predict the (target sequence + the output from encoder model) with decoder model\n",
    "            dec_outputs , h , c = dec_model.predict([empty_target_seq] + states_values)\n",
    "            # Get the index for sampled word using the dec_outputs\n",
    "            # dec_outputs is a numpy array of the shape (sample, timesteps, VOCAB_SIZE)\n",
    "            # To start, we can just pick the word with the higest probability - greedy search\n",
    "            sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
    "            \n",
    "            # Initialize the sampled word with None\n",
    "            sampled_word = None\n",
    "\n",
    "            # Iterate through words and their indexes\n",
    "            for word, index in tokenizer.word_index.items() :\n",
    "\n",
    "                # If the index is equal to sampled word's index\n",
    "                if sampled_word_index == index :\n",
    "\n",
    "                    # Add the word to the decoded string\n",
    "                    decoded_translation.append(word)\n",
    "\n",
    "                    # Update the sampled word\n",
    "                    sampled_word = word\n",
    "\n",
    "            # If sampled word is equal to \"end\" OR the length of decoded string is more that what is allowed\n",
    "            if sampled_word == 'endtoken' or len(decoded_translation) > TEXT_LIMIT:\n",
    "\n",
    "                # Make the stop_condition to true\n",
    "                stop_condition = True\n",
    "\n",
    "            # Initialize back the target sequence to zero - array([[0.]])    \n",
    "            empty_target_seq = np.zeros(shape = (1, 1))  \n",
    "\n",
    "            # Update the target sequence with index of \"start\"\n",
    "            empty_target_seq[0, 0] = sampled_word_index\n",
    "\n",
    "            # Get the state values\n",
    "            states_values = [h, c] \n",
    "\n",
    "            # Print the decoded string\n",
    "        print(' '.join(decoded_translation[:-1]))\n",
    "except KeyboardInterrupt:\n",
    "    print('Ending conversational agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqeHbygA8108"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CIS519_Finial_Project.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
