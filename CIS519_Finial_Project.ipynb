{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIS519_Finial_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Preprocess"
      ],
      "metadata": {
        "id": "aLEFunpdhlzL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Data Collection \n",
        "\n",
        "Download the datasets from [Cornell Movie Datasets Website](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) and unzip the data into txt files.\n",
        "\n"
      ],
      "metadata": {
        "id": "zw0TQGqWh_cY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -nc \"http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\"\n",
        "! unzip cornell_movie_dialogs_corpus.zip\n",
        "! rm cornell_movie_dialogs_corpus.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQ08H1wEiHzD",
        "outputId": "446f43b5-e33e-4a72-8fdc-a01053dfa42e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-19 15:46:44--  http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9916637 (9.5M) [application/zip]\n",
            "Saving to: ‘cornell_movie_dialogs_corpus.zip’\n",
            "\n",
            "cornell_movie_dialo 100%[===================>]   9.46M  26.7MB/s    in 0.4s    \n",
            "\n",
            "2022-04-19 15:46:44 (26.7 MB/s) - ‘cornell_movie_dialogs_corpus.zip’ saved [9916637/9916637]\n",
            "\n",
            "Archive:  cornell_movie_dialogs_corpus.zip\n",
            "   creating: cornell movie-dialogs corpus/\n",
            "  inflating: cornell movie-dialogs corpus/.DS_Store  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/cornell movie-dialogs corpus/\n",
            "  inflating: __MACOSX/cornell movie-dialogs corpus/._.DS_Store  \n",
            "  inflating: cornell movie-dialogs corpus/chameleons.pdf  \n",
            "  inflating: __MACOSX/cornell movie-dialogs corpus/._chameleons.pdf  \n",
            "  inflating: cornell movie-dialogs corpus/movie_characters_metadata.txt  \n",
            "  inflating: cornell movie-dialogs corpus/movie_conversations.txt  \n",
            "  inflating: cornell movie-dialogs corpus/movie_lines.txt  \n",
            "  inflating: cornell movie-dialogs corpus/movie_titles_metadata.txt  \n",
            "  inflating: cornell movie-dialogs corpus/raw_script_urls.txt  \n",
            "  inflating: cornell movie-dialogs corpus/README.txt  \n",
            "  inflating: __MACOSX/cornell movie-dialogs corpus/._README.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Data Cleaning & Wrangling\n",
        "\n",
        "Clean the data and convert it into the form of a dialog."
      ],
      "metadata": {
        "id": "WwM5YMC1QXH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# open dialog files\n",
        "movie_lines = open('cornell movie-dialogs corpus/movie_lines.txt', encoding='utf-8',errors='ignore').read().split('\\n')\n",
        "movie_conversations = open('cornell movie-dialogs corpus/movie_conversations.txt', encoding='utf-8',errors='ignore').read().split('\\n')"
      ],
      "metadata": {
        "id": "Bbp11GwnmtYG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build a dictionary to record (line_number, dialog) mappings\n",
        "line_to_dialog = {}\n",
        "for line in movie_lines:\n",
        "  line_splited = line.split(' +++$+++ ')\n",
        "  line_to_dialog[line_splited[0]] = line_splited[-1]"
      ],
      "metadata": {
        "id": "yaSnbTgHnnwH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build dialog fragments\n",
        "dialog_fragments = []\n",
        "for conversation in movie_conversations:\n",
        "  ## convert u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197'] to 'L194', 'L195', 'L196', 'L197'\n",
        "  dialog_instance = conversation.split(' +++$+++ ')[-1][1:-1]\n",
        "  ## convert 'L194', 'L195', 'L196', 'L197' to ['L194', 'L195', 'L196', 'L197']\n",
        "  dialog_fragments.append(dialog_instance.replace(\"'\", \" \").replace(\",\",\"\").split())"
      ],
      "metadata": {
        "id": "ktu9maMdp5YE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert dialog fragments into (question, answer) pairs\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "for frag in dialog_fragments:\n",
        "    for i in range(1, len(frag)):\n",
        "        questions.append(line_to_dialog[frag[i-1]])\n",
        "        answers.append(line_to_dialog[frag[i]])"
      ],
      "metadata": {
        "id": "n6ZyUbPJk6xE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show questions and answers pairs\n",
        "def qa_show(num):\n",
        "  for i in range(num):\n",
        "    print('-------------------------------------------------\\n')\n",
        "    print(f'Dialog A: {questions[i]}\\n')\n",
        "    print(f'Dialog B: {answers[i]}\\n')\n",
        "\n",
        "qa_show(5)"
      ],
      "metadata": {
        "id": "sAWPaVbtmdX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Text Preprocessing\n",
        "\n",
        "Preprocess the texts.\n"
      ],
      "metadata": {
        "id": "ixNAnkQiQ6wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Global variables\n",
        "TEXT_LIMIT = 13"
      ],
      "metadata": {
        "id": "XkeekSzCSEfn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter out long dialogs\n",
        "def filter_long_texts(questions, answers, limit):\n",
        "    short_questions = []\n",
        "    short_answers = []\n",
        "    for i in range(len(questions)):\n",
        "        # if len(questions[i]) <= TEXT_LIMIT and len(answers[i]) <=TEXT_LIMIT:\n",
        "        if len(questions[i].split()) <= TEXT_LIMIT:\n",
        "            short_questions.append(questions[i])\n",
        "            short_answers.append(answers[i])\n",
        "    return short_questions, short_answers"
      ],
      "metadata": {
        "id": "9WZ0gkXaSNNK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_questions, filtered_answers = filter_long_texts(questions, answers, TEXT_LIMIT)"
      ],
      "metadata": {
        "id": "Ix7oU7nabaun"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean the texts\n",
        "import re\n",
        "\n",
        "replacement_patterns = [\n",
        "  (r'won\\'t', 'will not'),\n",
        "  (r'can\\'t', 'cannot'),\n",
        "  (r'i\\'m', 'i am'),\n",
        "  (r'ain\\'t', 'is not'),\n",
        "  (r'(\\w+)\\'ll', '\\g<1> will'),\n",
        "  (r'(\\w+)n\\'t', '\\g<1> not'),\n",
        "  (r'(\\w+)\\'ve', '\\g<1> have'),\n",
        "  (r'(\\w+)\\'s', '\\g<1> is'),\n",
        "  (r'(\\w+)\\'re', '\\g<1> are'),\n",
        "  (r'(\\w+)\\'d', '\\g<1> would'),\n",
        "]\n",
        "\n",
        "class TextCleaner(object):\n",
        "  def __init__(self, patterns=replacement_patterns):\n",
        "    self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
        "    \n",
        "  def replace(self, text):\n",
        "    s = text\n",
        "    for (pattern, replace) in self.patterns:\n",
        "      s = re.sub(pattern, replace, s)\n",
        "    return s\n",
        "\n",
        "cleaner = TextCleaner()\n",
        "\n",
        "# Function for preprocessing the given text\n",
        "def preprocess_text(text):\n",
        "    \n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Decontracting the text (e.g. it's -> it is)\n",
        "    text = cleaner.replace(text)\n",
        "    \n",
        "    # Remove the punctuation\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r\"[ ]+\", \" \", text)\n",
        "    \n",
        "    return text"
      ],
      "metadata": {
        "id": "pY6jluMCSsn8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_questions = [preprocess_text(que) for que in filtered_questions]\n",
        "cleaned_answers = [' '.join(ans.split()[:TEXT_LIMIT-2]) for ans in filtered_answers]\n",
        "\n",
        "del(questions, answers, filtered_questions, filtered_answers)\n",
        "\n",
        "for i in range(5):\n",
        "    print('-------------------------------------------------\\n')\n",
        "    print(f'Dialog A: {cleaned_questions[i]}\\n')\n",
        "    print(f'Dialog B: {cleaned_answers[i]}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcPRHsfIXDtc",
        "outputId": "2a4561f0-d095-4834-c021-f9fbbb1cbfb0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: well i thought we would start with pronunciation if that is okay with you \n",
            "\n",
            "Dialog B: Not the hacking and gagging and spitting part. Please.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: not the hacking and gagging and spitting part please \n",
            "\n",
            "Dialog B: Okay... then how 'bout we try out some French cuisine. Saturday?\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: you are asking me out that is so cute what is your name again \n",
            "\n",
            "Dialog B: Forget it.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: no no it is my fault we did not have a proper introduction \n",
            "\n",
            "Dialog B: Cameron.\n",
            "\n",
            "-------------------------------------------------\n",
            "\n",
            "Dialog A: cameron \n",
            "\n",
            "Dialog B: The thing is, Cameron -- I'm at the mercy of a\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After preprocessing the dataset, we should add a start tag (e.g. `<start>`) and an end tag (e.g. `<end>`) to answers. Remember that we will only add these tags to answers and not questions.\n",
        "\n",
        "When Seq2Seq model is generating the word answers, we can first send it the `<start>` to begin the word generation. When `<end>` is generated, we will stop the iteration.\n"
      ],
      "metadata": {
        "id": "JEuvFeGuF9bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_answers = [\"starttoken \" + ans + \" endtoken\" for ans in cleaned_answers]"
      ],
      "metadata": {
        "id": "ETc_LE9sXX0O"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trim the data in case of running out of RAM\n",
        "cleaned_questions = cleaned_questions[:30000]\n",
        "cleaned_answers = cleaned_answers[:30000]"
      ],
      "metadata": {
        "id": "lakxrfs9H0uQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Input Encoding\n",
        "\n",
        "Convet String input into numerical values\n"
      ],
      "metadata": {
        "id": "Pj2VSg6ento0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import preprocessing, utils\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "6B4lo51bH6Fs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_WORDS = 3500"
      ],
      "metadata": {
        "id": "CJUf6CjJ37u7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = preprocessing.text.Tokenizer(num_words = NUM_WORDS, oov_token='nulltoken')\n",
        "\n",
        "# Fit the tokenizer to questions and answers\n",
        "tokenizer.fit_on_texts(cleaned_questions + cleaned_answers)\n",
        "\n",
        "# Get the total vocab size\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "# display(tokenizer.word_index)\n",
        "print( 'VOCAB SIZE : {}'.format(VOCAB_SIZE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjSxe5uGIH6q",
        "outputId": "ed256e97-3f6c-407f-f305-bebc54a9616d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VOCAB SIZE : 14987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index[\"starttoken\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGTAwFiL9Wq3",
        "outputId": "d45d2a0c-0397-4ef5-9958-2ca5bda07bdf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### encoder input data\n",
        "\n",
        "# Tokenize the questions\n",
        "tokenized_questions = tokenizer.texts_to_sequences(cleaned_questions)\n",
        "\n",
        "# Get the length of longest sequence\n",
        "# maxlen_questions = max([len(x) for x in tokenized_questions])\n",
        "\n",
        "# Pad the sequences\n",
        "padded_questions = pad_sequences(tokenized_questions, maxlen=TEXT_LIMIT, padding='post')\n",
        "\n",
        "# Convert the sequences into array\n",
        "encoder_input_data = np.array(padded_questions)\n",
        "# print(encoder_input_data[0])\n",
        "print(encoder_input_data.shape, TEXT_LIMIT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCeEQiMwIZas",
        "outputId": "dd38922f-dd4d-4f57-c6d7-2e94f4e62d4d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30000, 13) 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### decoder input data\n",
        "\n",
        "# Tokenize the answers\n",
        "tokenized_answers = tokenizer.texts_to_sequences(cleaned_answers)\n",
        "\n",
        "# Get the length of longest sequence\n",
        "# maxlen_answers = max([len(x) for x in tokenized_answers])\n",
        "\n",
        "# Pad the sequences\n",
        "padded_answers = pad_sequences(tokenized_answers, maxlen=TEXT_LIMIT, padding='post')\n",
        "\n",
        "# Convert the sequences into array\n",
        "decoder_input_data = np.array(padded_answers)\n",
        "\n",
        "print(decoder_input_data.shape, TEXT_LIMIT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoKg2PHoIkcH",
        "outputId": "092abd9d-dba9-4bc6-e23e-ec6a01e7f418"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30000, 13) 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### decoder_output_data\n",
        "\n",
        "# Iterate through index of tokenized answers\n",
        "for i in range(len(tokenized_answers)):\n",
        "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
        "\n",
        "# Pad the tokenized answers\n",
        "padded_answers = pad_sequences(tokenized_answers, maxlen = TEXT_LIMIT, padding = 'post')\n",
        "\n",
        "# One hot encode\n",
        "onehot_answers = utils.to_categorical(padded_answers, NUM_WORDS+1)"
      ],
      "metadata": {
        "id": "SXi3CTOMI8AX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to numpy array\n",
        "decoder_output_data = np.array(onehot_answers)\n",
        "# print(decoder_output_data[0])\n",
        "print(decoder_output_data.shape)\n",
        "\n",
        "del (onehot_answers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnyhJ7VC4-YY",
        "outputId": "03520b6b-3622-4ef4-c6b8-8c636972f595"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30000, 13, 3501)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the libraries\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.activations import softmax\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "ZmOuXZ6vJJcj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper parameters\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "VOCAB_SIZE = NUM_WORDS + 1"
      ],
      "metadata": {
        "id": "SMch0EsnkmMN"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Encoder Input\n",
        "embed_dim = 50\n",
        "num_lstm = 400\n",
        "\n",
        "# Input for encoder\n",
        "encoder_inputs = Input(shape = (None, ), name='encoder_inputs')\n",
        "\n",
        "# Embedding layer\n",
        "# Why mask_zero = True? https://www.tensorflow.org/guide/keras/masking_and_padding\n",
        "encoder_embedding = Embedding(input_dim = VOCAB_SIZE, output_dim = embed_dim, mask_zero = True, name='encoder_embedding')(encoder_inputs)\n",
        "\n",
        "# LSTM layer (that returns states in addition to output)\n",
        "encoder_outputs, state_h, state_c = LSTM(units = num_lstm, return_state = True, name='encoder_lstm')(encoder_embedding)\n",
        "\n",
        "# Get the states for encoder\n",
        "encoder_states = [state_h, state_c]"
      ],
      "metadata": {
        "id": "Wwi7M2Nmk1oR"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Decoder\n",
        "\n",
        "# Input for decoder\n",
        "decoder_inputs = Input(shape = (None,  ), name='decoder_inputs')\n",
        "\n",
        "# Embedding layer\n",
        "decoder_embedding = Embedding(input_dim = VOCAB_SIZE, output_dim = embed_dim , mask_zero = True, name='decoder_embedding')(decoder_inputs)\n",
        "\n",
        "# LSTM layer (that returns states and sequences as well)\n",
        "decoder_lstm = LSTM(units = num_lstm , return_state = True , return_sequences = True, name='decoder_lstm')\n",
        "\n",
        "# Get the output of LSTM layer, using the initial states from the encoder\n",
        "decoder_outputs, _, _ = decoder_lstm(inputs = decoder_embedding, initial_state = encoder_states)\n",
        "\n",
        "# Dense layer\n",
        "decoder_dense = Dense(units = VOCAB_SIZE, activation = softmax, name='decoder_outputs') \n",
        "\n",
        "# Get the output of Dense layer\n",
        "output = decoder_dense(decoder_outputs)"
      ],
      "metadata": {
        "id": "wbyKdEcXk6d6"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "model = Model([encoder_inputs, decoder_inputs], output)"
      ],
      "metadata": {
        "id": "xK6yhoCtlDYK"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', metrics=['acc'], loss = \"categorical_crossentropy\")"
      ],
      "metadata": {
        "id": "T_nMCQ3VlHoO"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "bHJ7aT6GlH4x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5d49285-e393-4cc8-dc20-b70ea2d291f1"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " encoder_embedding (Embedding)  (None, None, 50)     175050      ['encoder_inputs[0][0]']         \n",
            "                                                                                                  \n",
            " decoder_embedding (Embedding)  (None, None, 50)     175050      ['decoder_inputs[0][0]']         \n",
            "                                                                                                  \n",
            " encoder_lstm (LSTM)            [(None, 400),        721600      ['encoder_embedding[0][0]']      \n",
            "                                 (None, 400),                                                     \n",
            "                                 (None, 400)]                                                     \n",
            "                                                                                                  \n",
            " decoder_lstm (LSTM)            [(None, None, 400),  721600      ['decoder_embedding[0][0]',      \n",
            "                                 (None, 400),                     'encoder_lstm[0][1]',           \n",
            "                                 (None, 400)]                     'encoder_lstm[0][2]']           \n",
            "                                                                                                  \n",
            " decoder_outputs (Dense)        (None, None, 3501)   1403901     ['decoder_lstm[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,197,201\n",
            "Trainable params: 3,197,201\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(\n",
        "    x = {\"encoder_inputs\": encoder_input_data, \"decoder_inputs\": decoder_input_data}, \n",
        "    y = {'decoder_outputs': decoder_output_data}, \n",
        "    batch_size = BATCH_SIZE, \n",
        "    epochs = EPOCHS\n",
        ") "
      ],
      "metadata": {
        "id": "IfVO8dDhlNeH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5f08152-316c-4bf4-e01d-7dd2be23ff72"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "938/938 [==============================] - 334s 355ms/step - loss: 2.7956 - acc: 0.2925\n",
            "Epoch 2/10\n",
            "938/938 [==============================] - 333s 355ms/step - loss: 2.6918 - acc: 0.3050\n",
            "Epoch 3/10\n",
            "938/938 [==============================] - 333s 355ms/step - loss: 2.6154 - acc: 0.3127\n",
            "Epoch 4/10\n",
            "938/938 [==============================] - 333s 355ms/step - loss: 2.5450 - acc: 0.3193\n",
            "Epoch 5/10\n",
            "938/938 [==============================] - 331s 353ms/step - loss: 2.4811 - acc: 0.3250\n",
            "Epoch 6/10\n",
            "938/938 [==============================] - 328s 350ms/step - loss: 2.4197 - acc: 0.3305\n",
            "Epoch 7/10\n",
            "938/938 [==============================] - 329s 351ms/step - loss: 2.3584 - acc: 0.3358\n",
            "Epoch 8/10\n",
            "938/938 [==============================] - 327s 349ms/step - loss: 2.2980 - acc: 0.3416\n",
            "Epoch 9/10\n",
            "938/938 [==============================] - 328s 350ms/step - loss: 2.2387 - acc: 0.3484\n",
            "Epoch 10/10\n",
            "938/938 [==============================] - 328s 350ms/step - loss: 2.1806 - acc: 0.3547\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2cf76ffad0>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(filepath=\"weight.h5\")"
      ],
      "metadata": {
        "id": "6amEIRtjlo-w"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference\n",
        "\n",
        "Build the inference model(the same as training model) and load the trained weight. "
      ],
      "metadata": {
        "id": "sMtI7RRq6B_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the final model\n",
        "model.load_weights('weight.h5') \n",
        "print(\"Model Weight Loaded!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "373cZtwERQpm",
        "outputId": "e08200b0-3d87-44b0-d656-f5a3b87e0789"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Weight Loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for making inference\n",
        "def make_inference_models():\n",
        "    \n",
        "    # Create a model that takes encoder's input and outputs the states for encoder\n",
        "    encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
        "    \n",
        "    # Create two inputs for decoder which are hidden state (or state h) and cell state (or state c)\n",
        "    decoder_state_input_h = Input(shape = (num_lstm, ), name='decoder_state_input_h')\n",
        "    decoder_state_input_c = Input(shape = (num_lstm, ), name='decoder_state_input_c')\n",
        "    \n",
        "    # Store the two inputs for decoder inside a list\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    \n",
        "    # Pass the inputs through LSTM layer you have created before\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state = decoder_states_inputs)\n",
        "    \n",
        "    # Store the outputted hidden state and cell state from LSTM inside a list\n",
        "    decoder_states = [state_h, state_c]\n",
        "\n",
        "    # Pass the output from LSTM layer through the dense layer you have created before\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Create a model that takes decoder_inputs and decoder_states_inputs as inputs and outputs decoder_outputs and decoder_states\n",
        "    decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
        "                          [decoder_outputs] + decoder_states)\n",
        "    \n",
        "    return encoder_model , decoder_model"
      ],
      "metadata": {
        "id": "eH4GLMnk60hE"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for converting strings to tokens\n",
        "def str_to_tokens(sentence: str, tokenizer, maxlen_questions=TEXT_LIMIT):\n",
        "\n",
        "    # Lowercase the sentence and split it into words\n",
        "    words = sentence.lower().split()\n",
        "\n",
        "    tokens_list = tokenizer.texts_to_sequences([sentence])\n",
        "\n",
        "    # Pad the sequences to be the same length\n",
        "    return pad_sequences(tokens_list , maxlen = maxlen_questions, padding = 'post')"
      ],
      "metadata": {
        "id": "7MEWuJdn7H5M"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model for inference\n",
        "enc_model , dec_model = make_inference_models()"
      ],
      "metadata": {
        "id": "OD5ePfvV63vc"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the number of times you want to ask question\n",
        "try:\n",
        "    for _ in range(5):\n",
        "\n",
        "        # Get the input and predict it with the encoder model\n",
        "        encoder_inputs = str_to_tokens(preprocess_text(input('Enter question : ')), tokenizer)\n",
        "        states_values = enc_model.predict(encoder_inputs)\n",
        "\n",
        "        # Initialize the decoder input sequence with the starttoken index\n",
        "        # Reshape this to be a (1, 1) array, since it is a sequence of 1 sample for 1 timestep\n",
        "        empty_target_seq = np.zeros((1, 1))\n",
        "\n",
        "        # Update the target sequence with index of \"start\"\n",
        "        empty_target_seq[0, 0] = tokenizer.word_index[\"starttoken\"]\n",
        "        # Initialize the stop condition with False\n",
        "        stop_condition = False\n",
        "\n",
        "        # Initialize the decoded words with an empty string\n",
        "        decoded_translation = []\n",
        "\n",
        "        # While stop_condition is false\n",
        "        while not stop_condition :\n",
        "\n",
        "            # Predict the (target sequence + the output from encoder model) with decoder model\n",
        "            dec_outputs , h , c = dec_model.predict([empty_target_seq] + states_values)\n",
        "            # Get the index for sampled word using the dec_outputs\n",
        "            # dec_outputs is a numpy array of the shape (sample, timesteps, VOCAB_SIZE)\n",
        "            # To start, we can just pick the word with the higest probability - greedy search\n",
        "            sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "            \n",
        "            # Initialize the sampled word with None\n",
        "            sampled_word = None\n",
        "\n",
        "            # Iterate through words and their indexes\n",
        "            for word, index in tokenizer.word_index.items() :\n",
        "\n",
        "                # If the index is equal to sampled word's index\n",
        "                if sampled_word_index == index :\n",
        "\n",
        "                    # Add the word to the decoded string\n",
        "                    decoded_translation.append(word)\n",
        "\n",
        "                    # Update the sampled word\n",
        "                    sampled_word = word\n",
        "\n",
        "            # If sampled word is equal to \"end\" OR the length of decoded string is more that what is allowed\n",
        "            if sampled_word == 'endtoken' or len(decoded_translation) > TEXT_LIMIT:\n",
        "\n",
        "                # Make the stop_condition to true\n",
        "                stop_condition = True\n",
        "\n",
        "            # Initialize back the target sequence to zero - array([[0.]])    \n",
        "            empty_target_seq = np.zeros(shape = (1, 1))  \n",
        "\n",
        "            # Update the target sequence with index of \"start\"\n",
        "            empty_target_seq[0, 0] = sampled_word_index\n",
        "\n",
        "            # Get the state values\n",
        "            states_values = [h, c] \n",
        "\n",
        "            # Print the decoded string\n",
        "        print(' '.join(decoded_translation[:-1]))\n",
        "except KeyboardInterrupt:\n",
        "    print('Ending conversational agent')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnuR5dmZ66h1",
        "outputId": "70aa47d9-db81-4291-aed6-25883d72bca6"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter question : how are you\n",
            "[[39 16  4  0  0  0  0  0  0  0  0  0  0]]\n",
            "i don't know\n",
            "Enter question : what a pity\n",
            "[[  11    9 1308    0    0    0    0    0    0    0    0    0    0]]\n",
            "i don't know\n",
            "Enter question : i don't know\n",
            "[[ 5 13 12 24  0  0  0  0  0  0  0  0  0]]\n",
            "you don't have to be a nulltoken\n",
            "Enter question : to be a what\n",
            "[[ 8 31  9 11  0  0  0  0  0  0  0  0  0]]\n",
            "you know what i think\n",
            "Enter question : probably\n",
            "[[319   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
            "the nulltoken\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kqeHbygA8108"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}